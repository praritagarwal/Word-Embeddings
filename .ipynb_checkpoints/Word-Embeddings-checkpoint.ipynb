{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create and train an embedding layer for the words appearing in a text data. We will then train a simple DNN based model to do sentiment analysis on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exercise 13.10 in [this](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "\n",
    "  - a. Download the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
    "  \n",
    "    \n",
    "  - b. Split the test set into a validation set (15,000) and a test set (10,000).\n",
    "  \n",
    "  \n",
    "  - c. Use tf.data to create an efficient dataset for each set.\n",
    "  \n",
    "  \n",
    "  - d. Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method.\n",
    "  \n",
    "  \n",
    "  - e. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n",
    "  \n",
    "  \n",
    "  - f. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.\n",
    "\n",
    "\n",
    "  - g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.1.0\n",
      "keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/prarit/MachineLearningProjects/Word-Embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('cwd: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good tutorial on using wget: https://www.tecmint.com/download-and-extract-tar-files-with-one-command/\n",
    "# turn off verbose output of wget using the flag -nv : https://shapeshed.com/unix-wget/#how-to-turn-off-verbose-output \n",
    "!wget -c -nv http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -o - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncompress the downloaded files\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tensorflow also provides this dataset: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'aclImdb',\n",
       " 'Word-Embeddings.ipynb',\n",
       " 'aclImdb_v1.tar.gz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the files in the current working directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that aclImdb_v1.tar.gz was extracted to a folder called aclImdb. Let's see the contents of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of aclImdb are: \n",
      "['imdb.vocab', 'train', 'README', 'imdbEr.txt', 'test']\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd() , 'aclImdb')\n",
    "contents = os.listdir(path)\n",
    "print('The contents of aclImdb are: \\n{}'.format(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a README file in aclImdb, let us read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read README\n",
    "filepath = os.path.join(path, 'README')\n",
    "with open(filepath, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From README, we find that the train/test folder contains a 'pos' folder for positive reviews and a 'neg' for negative reviews along with .txt files containing urls of positive and negative reviews respectively. There are also some other files for bag-of-words features etc. \n",
    "\n",
    "Each train/test folder contains a total of 25000 reviews of which 12500 are positive reviews and 12500 are negative reviews.\n",
    "\n",
    "Let's verify the above about the 'train' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents of the train folder: \n",
      "['unsup', 'urls_pos.txt', 'pos', 'labeledBow.feat', 'unsupBow.feat', 'neg', 'urls_neg.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(path,'train')\n",
    "print('contents of the train folder: \\n{}'.format(os.listdir(train_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a dataset, we need the path to all the reviews. We can create the corresponding list of paths using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with positive reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the positive reviews in the training set\n",
    "train_pos_path = os.path.join(train_path, 'pos', '*.txt')\n",
    "train_pos_reviews = glob.glob(train_pos_path)\n",
    "print('No. of train-set files with positive reviews: {}'.format(len(train_pos_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the negative reviews in the training set\n",
    "train_neg_path = os.path.join(train_path, 'neg', '*.txt')\n",
    "train_neg_reviews = glob.glob(train_neg_path)\n",
    "print('No. of train-set files with negative reviews: {}'.format(len(train_neg_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us give a brief look at a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I watch them all.<br /><br />It's not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it's completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don't watch. 'How she move.' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It's just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I'm a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn't all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.\n"
     ]
    }
   ],
   "source": [
    "file = train_pos_reviews[0]\n",
    "with open(file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will like to load an preprocess all the data using tensorflow's data API, therefore let us quickly see how to read the same file as above but this time by using tensorflow's [TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'I watch them all.<br /><br />It\\'s not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it\\'s completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don\\'t watch. \\'How she move.\\' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It\\'s just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I\\'m a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn\\'t all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "pos_fl0 = tf.data.TextLineDataset(file)\n",
    "for item in pos_fl0:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As expected, we see that the pos_fl0 contains a single item and it's value output matches the output of the previous code cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having, learnt how to use tf.data.TextLineData() method, we can now starting preprocessing the data. In order to do this, we notice that the review contains punctuation marks and html line break tags etc. We will have to write a preprocessing function to get rid of these. Additionally, we will also change all alphabets to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing line-brk tags\n",
    "\n",
    "This can be very simply done by using the .replace() method of python strings. We can therefore use it to replace all occurrances of the line-break tag with a space. In tensorflow, the equivalent method is [tf.strings.regex_replace()](https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.strings.regex_replace\n",
    "Note that 'regex' in regex_replace() stands for [\"regular expression\"](https://docs.python.org/3/howto/regex.html). For e.g. the following will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('hello','e','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the following will throw an error: tf.strings.regex_replace('h(llo', '(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because \"(\" is a metacharacter. To match and replace meta-characters, we must prepend a backslash before them. This can be done as follows: '\\\\\\\\' + char. \n",
    "\n",
    "The previous code cell can now be made to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The error thrown by this code cell is intentional\n",
    "tf.strings.regex_replace('h(llo', '\\\\'+'(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h(llo','\\(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imp: \n",
    "\n",
    "Note that backslash itself is also a  meta-character. To search and replace backslash, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h\\llo', '\\\\'+'\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use '\\\\\\\\' and NOT '\\\\' to search and replace a backslash\n",
    "tf.strings.regex_replace('h\\llo', '\\\\\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try three different ways of removing punctuations from a tensorflow string and compare their timings:\n",
    "\n",
    "1) punc_filter_and_to_lower1: use tf.strings.unicode_decode() to convert all the characters in the string into an array of their ascii codes. We then loop through this array, skipping over the places where the entry matches the ascii code of a punctuation. Finally we call tf.strings.unicode_encode() on this array to convert the ascii codes back to characters, thereby obtaining a string with all the punctuations stripped. \n",
    "\n",
    "\n",
    "2) punc_filter_and_to_lower2: use tf.regex_replace() to search and replace each punctuation by empty space. Pay special attention to prepend backslash in order to be able to use meta-characters in regex_replace.\n",
    "\n",
    "\n",
    "3) punc_filter_and_to_lower3: simply extract the python string using its .numpy() method. Then simple iterate through the characters of the string, skipping over the punctutations. Join the resulting list of character using .join() method. \n",
    "\n",
    "4) punc_filter_and_to_lower4: Use [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) by setting it's \"alphanum_only\" arg to True. This way it will only parse alpha-numeric characters in the text and to split the text at occurrances of non-alphanumeric characters. Since whitespace is a non-alpha-numeric character, the output will largely consist of a list of words in the text.  The caeat with this approach is that words with apostrophe in them such as \" don't \" will be split into two words: \"don\" and \"t\". Also, [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) does NOT consider underscores as non-alpha-numeric, so underscores do not get removed form the text. On the other hand since, the tokenizer already generates a list of words, we use this list to generate a vocobulary of words in the dataset at this step it-self, making the preprocessing faster.\n",
    "\n",
    "There timing on the first review in the training set was as follows:\n",
    "\n",
    "1) punc_filter_and_to_lower1: Wall time: ~ 3 s\n",
    "\n",
    "2) punc_filter_and_to_lower2: Wall time: ~ 8 ms\n",
    "\n",
    "3) punc_filter_and_to_lower3: Wall time: ~ 6 ms\n",
    "\n",
    "4) punc_filter_and_to_lower4: Wall time: ~ 6 ms (when not udating a vocabulary of words) \n",
    "\n",
    "5) 4) punc_filter_and_to_lower4: Wall time: ~ 7 ms (when also udating a vocabulary of words) \n",
    "\n",
    "Clearly, the last function is the fastest with the first one being extremely slow (takes several secs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations before utf encoding: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "punctuations after utf encoding: \n",
      "[ 33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  58  59  60\n",
      "  61  62  63  64  91  92  93  94  95  96 123 124 125 126]\n"
     ]
    }
   ],
   "source": [
    "# list of punctuations\n",
    "punc_ls = string.punctuation\n",
    "print('punctuations before utf encoding: {}'.format(punc_ls))\n",
    "punc_ls2 = tf.strings.unicode_decode(punc_ls, input_encoding = 'UTF-8')\n",
    "print('punctuations after utf encoding: \\n{}'.format(punc_ls2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.unicode_encode(), tf.strings.unicode_decode()\n",
    "def punc_filter_and_to_lower1(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = tf.strings.unicode_decode(st2, input_encoding = 'utf-8')\n",
    "    st2 = tf.strings.unicode_encode([char for char in st2 if char not in punc_ls2], output_encoding = 'UTF-8')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.47 s, sys: 38.5 ms, total: 3.51 s\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower1(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.regex_replace()\n",
    "def punc_filter_and_to_lower2(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ')\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    for punc in punc_ls:\n",
    "        # V.imp: to replace meta-characters we prepend a backslash to them \n",
    "        # infact we can also prepend a backslach before all the characters\n",
    "        # to prepend a backslash before a character we do: '\\\\' + char\n",
    "        st2 = tf.strings.regex_replace(st2, '\\\\'+ punc, ' ')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.41 ms, sys: 3.71 ms, total: 10.1 ms\n",
      "Wall time: 9.43 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower2(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all  it s not better than the amazing ones   strictly ballroom    shall we dance    japanese version   but it s completely respectable and pleasingly different in parts  i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting  for example  the  name should scream don t watch   how she move   since when can movie titles ignore grammar    there is nothing inherently incorrect about caribbean english grammar  it s just not canadian standard english grammar  comments about the dialogue seem off to me  i put on the subtitles because i m a canadian standard english speaker  so i just automatically assumed that i would have trouble understanding all of it  it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american  i loved that this movie was set in toronto and  in fact  wish it was even more clearly set there  i loved that the heroine was so atypically cast  i enjoyed the stepping routines  i liked the driven mum character  i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies  in summary  if you tend to like dance movies  then this is a decent one  if you have superiority issues about the grammar of the english standard you grew up speaking  your narrow mind may have difficulty enjoying this movie '>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using .join() method in python string class\n",
    "def punc_filter_and_to_lower3(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = st2.numpy().decode('utf-8')\n",
    "    st2 = ''.join([char for char in st2 if char not in punc_ls])\n",
    "    \n",
    "    st2 = st2.lower()\n",
    "    \n",
    "    st2 = tf.convert_to_tensor(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.18 ms, sys: 0 ns, total: 5.18 ms\n",
      "Wall time: 5.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower3(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tfds.features.text.tokenizer()\n",
    "# we will also simultaneously generate a vocabulary in this step\n",
    "import tensorflow_datasets as tfds\n",
    "def punc_filter_and_to_lower4(st, vocab = None):\n",
    "    ''' vocab: vocabulary to update with the words in st'''\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.lower(tf.strings.regex_replace(st, line_brk_tag, ' '))\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    tokenizer = tfds.features.text.Tokenizer()\n",
    "    words = tokenizer.tokenize(st2.numpy()) # note that the inpout has to be a python string NOT as tensor \n",
    "                                            # The output is a list NOT a tensor\n",
    "                                            # https://stackoverflow.com/questions/56665868/tensor-numpy-not-working-in-tensorflow-data-dataset-throws-the-error-attribu\n",
    "    st2 = tf.strings.join(words, ' ')\n",
    "    \n",
    "    if type(vocab) == set:\n",
    "        vocab.update(words)\n",
    "        return st2, vocab\n",
    "    \n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 0 ns, total: 10.3 ms\n",
      "Wall time: 9.82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower4(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all it s not better than the amazing ones _strictly ballroom_ _shall we dance _ japanese version but it s completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream don t watch how she move since when can movie titles ignore grammar there is nothing inherently incorrect about caribbean english grammar it s just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because i m a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 ms, sys: 4 ms, total: 7.91 ms\n",
      "Wall time: 7.42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test the timing of punc_filter_and_to_lower4 when simultaneously updating a dictionary\n",
    "vocab2 = set([])\n",
    "for item in pos_fl0:\n",
    "    lst, vocab2 = punc_filter_and_to_lower4(item, vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary based on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be easily done using python's [set()](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset) container.: basically, split each text in the training instance into words and update the set with this list. This will add any new words in the text to the set. In the end, the set will contain all the unique words in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have also implemented the above idea in our function punc_filter_and_to_lower4() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a vocabulary using the set() container\n",
    "vocab1 = set([]) # python set for containing unique words in the training dataset\n",
    "def vocab_builder1(strng):\n",
    "    # split the string into its words\n",
    "    words = tf.strings.split(strng)\n",
    "    # update vocab\n",
    "    vocab1.update(words.numpy())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 333 ms, sys: 98.7 ms, total: 432 ms\n",
      "Wall time: 772 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower3(item)\n",
    "    vocab_builder1(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocab_builder1() function as defined above takes about 300 ms. On the otherhand, in punc_filter_and_to_lower4() function,  we were able to get almost the same result (upto the caveats mentioned in the previous section) by using tfds.features.text.Tokenizer(), in about 8 ms. Clearly, there is a difference of few order of magnitude between the time taken by the two routines with the difference between their output being quite tolerable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================="
     ]
    }
   ],
   "source": [
    "vocab = set([])\n",
    "train_filepaths = tf.data.Dataset.list_files([train_pos_path, train_neg_path])\n",
    "train_dataset = tf.data.TextLineDataset(train_filepaths)\n",
    "\n",
    "ctr = 0\n",
    "for item in train_dataset:\n",
    "    _ , vocab = punc_filter_and_to_lower4(item, vocab)\n",
    "    \n",
    "    ctr+=1\n",
    "    if ctr%255 ==0:\n",
    "        print(\"=\", end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reworking', 'presidency', 'pretentions', 'ensue', 'intensive']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick 5 random words from vocab to inspect it\n",
    "import random\n",
    "\n",
    "random.sample(vocab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words in vocabulary: 74893\n"
     ]
    }
   ],
   "source": [
    "print('no. of words in vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a lookup table based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "lookup_initializer = tf.lookup.KeyValueTensorInitializer( list(vocab), indices) \n",
    "num_oov_buckets = 50 \n",
    "lookup_table = tf.lookup.StaticVocabularyTable( initializer = lookup_initializer, \n",
    "                                               num_oov_buckets = num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map a text to integers using the above lookup table\n",
    "def text_to_integers(X):\n",
    "    X = tf.strings.split(X)\n",
    "    int_ls = lookup_table.lookup(X)\n",
    "    return int_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 8153 13566 17570 58115 66060 52440 58008  3883 38258  7716  9431 51613\n",
      " 41889 11171  5961 43916 33603 10418 69829 47403 46838 66060 52440  5694\n",
      "  3995 15172 69711 14390 34885 57812  8153 27013 25194 65854 60874 15172\n",
      "  8153 30189 48735 38501  7716 33058 10659 41662 34885 48735 38501 26151\n",
      "  6159 42575 19562 71448 49034  7716 27975 21201  2195 33688 17613 13566\n",
      " 17141 71807 26261 24590 39427 18118 74888  2003 58929 25698 41542  3359\n",
      " 33172 62406  9604 10659 12030 65854 25698 66060 52440 53520 58008 34481\n",
      " 67694 65854 25698 16946 10659  7716 70418 38158 36937 12777 47614  8153\n",
      " 13449 17076  7716 42363 25392  8153 50660 59616 34481 67694 65854 65075\n",
      " 53640  8153 53520 59090 62258  3623  8153 34836 31604  8008 36386 58115\n",
      " 38501 66060 66060 44262 17613 58115  3623 11232 15172 66060 24999 59616\n",
      " 19426 14390 25764 10310  7716 25824 27739  1073  8153 31604  4238 53849\n",
      " 53640  7487  8153 15470  3623 52415 74888 13522 49850 34885 73973 15172\n",
      " 34885 45944 55908 66060 13522 69685 51362 64563 49850 41542  8153 15470\n",
      "  3623  7716 29615 13522 53640   875 23166  8153 12474  7716  9771 22159\n",
      "  8153 22905  7716 23747 62237 65512  8153 51489  3623 13785 38501  7716\n",
      " 27365 34885  7716 74888 53849 56035 51362 21790 38258  3359 65147 38501\n",
      " 33603  1073 34885 67654 35363 59605 64044 12777 46800 33603  1073 67783\n",
      " 52415  3359 59616 44210 61848 35363 59605 31604 37287 27365 10659  7716\n",
      " 25698 38501  7716 65854 67694 59605   834 30676 61375 27667 68300 17651\n",
      " 57414 31604 62354 59852 52415 74888], shape=(246,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(74943, 100) dtype=float32, numpy=\n",
       "array([[0.8998282 , 0.6789483 , 0.12316942, ..., 0.6358366 , 0.8994671 ,\n",
       "        0.45826972],\n",
       "       [0.5889013 , 0.4919231 , 0.02740455, ..., 0.52845705, 0.67768025,\n",
       "        0.6385728 ],\n",
       "       [0.081043  , 0.2034707 , 0.85187685, ..., 0.4834875 , 0.86988854,\n",
       "        0.625527  ],\n",
       "       ...,\n",
       "       [0.13431859, 0.02604902, 0.40878165, ..., 0.5121021 , 0.91932213,\n",
       "        0.02689385],\n",
       "       [0.57968783, 0.71159744, 0.6222173 , ..., 0.9387883 , 0.293388  ,\n",
       "        0.60030365],\n",
       "       [0.9490136 , 0.14913893, 0.7684674 , ..., 0.14968002, 0.61564136,\n",
       "        0.59339464]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "initial_embeddings = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim] ) \n",
    "embedding_matrix = tf.Variable(initial_embeddings)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an embedding-vector from a list of word indices\n",
    "def indices_to_embeddings(idxs):\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix, idxs)\n",
    "    sqrt_num_words = tf.math.sqrt(tf.cast(tf.size(idxs), tf.float32)) # size is of type int32 but \n",
    "                                                                      # tf.math.sqrt has to have one of \n",
    "                                                                      # bfloat16, half, float32, \n",
    "                                                                      # float64, complex64, \n",
    "                                                                      # complex128.\n",
    "    mean_embedding = tf.math.reduce_mean(embeddings, axis = 0)\n",
    "    sentence_embedding = tf.multiply(sqrt_num_words, mean_embedding)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    item = indices_to_embeddings(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([7.7638135, 7.301368 , 8.844499 , 7.631126 , 7.344328 , 6.868389 ,\n",
       "       7.509292 , 8.086759 , 8.769074 , 8.292545 , 7.7935324, 8.103873 ,\n",
       "       8.877044 , 7.027474 , 7.8794703, 8.389485 , 7.5683856, 7.450319 ,\n",
       "       7.701784 , 7.2980886, 8.501083 , 8.567429 , 7.508877 , 8.1739   ,\n",
       "       8.527388 , 7.5273304, 8.188265 , 8.280729 , 8.019014 , 8.529987 ,\n",
       "       6.8208995, 7.9714823, 8.844231 , 8.529768 , 7.553604 , 8.742171 ,\n",
       "       7.9695415, 7.753538 , 8.430726 , 7.1172757, 8.177684 , 7.721968 ,\n",
       "       8.717742 , 7.1336017, 8.609493 , 8.755105 , 8.440074 , 8.341785 ,\n",
       "       8.755029 , 8.21621  , 7.1706805, 8.97794  , 7.82132  , 8.170061 ,\n",
       "       8.594518 , 7.3229656, 9.395067 , 7.9957795, 7.656941 , 8.609462 ,\n",
       "       7.2647676, 7.691476 , 7.1282344, 7.868062 , 7.0984254, 7.5056767,\n",
       "       7.5236177, 7.4100494, 8.205095 , 7.213056 , 7.734133 , 7.156183 ,\n",
       "       8.391964 , 7.4108186, 7.4185805, 7.721513 , 7.1482472, 8.072526 ,\n",
       "       8.208793 , 8.2399435, 8.424504 , 7.1990757, 7.1871433, 8.23654  ,\n",
       "       7.402993 , 7.7079477, 8.158678 , 7.3409724, 8.577539 , 8.70251  ,\n",
       "       7.1709547, 7.702089 , 7.305655 , 7.789259 , 7.9742045, 7.274939 ,\n",
       "       7.8722   , 7.292622 , 8.273968 , 8.192758 ], dtype=float32)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessing layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a preprocessing layer that accepts a sentence as an input, removes punctuations and line-brk tags form it, then converts it's words into indices and finally output an embedding vector corresponding to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_layer(keras.layer.Layers): \n",
    "    def __init__(self, vocab,  num_oov_buckets, embedding_len, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_len  = tf.size(vocab) \n",
    "        self.num_oov_buckets = num_oov_buckets\n",
    "        self.embedding_len = embedding_len\n",
    "        self.indices = tf.range(self.vocab_len, dtype = tf.int64)\n",
    "        self.lookup_intializer = tf.lookup.KeyValueTensorInitializer(list(self.vocab),\n",
    "                                                                     self.indices)\n",
    "        self.lookup_table = tf.lookup.StaticVocabularyTable(initializer = self.lookup_initializer, \n",
    "                                                            num_oov_buckets = self.num_oov_buckets)\n",
    "         \n",
    "    # build the embedding matrix \n",
    "    def build(self, batch_input_shape):\n",
    "        self.embedding_mat = self.add_weight(name = 'embedding matrix', \n",
    "                                             shape = [self.vocab_len + self.num_oov_buckets, self.embedding_len],\n",
    "                                             initializer = tf.random_uniform_initializer)\n",
    "        \n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    \n",
    "    # function to convert a sentence into a list of indices for its words\n",
    "    def text_to_integers(self, X):\n",
    "        X = tf.strings.split(X)\n",
    "        return lookup_table.lookup(X)\n",
    "    \n",
    "    # function to convert a list of indices into an embedding vector\n",
    "    def integers_to_embeddings(self, X):\n",
    "        X = tf.nn.embedding_lookup(self.embedding_mat, X)\n",
    "    \n",
    "    # define the call method\n",
    "    def call(self, X):\n",
    "        \n",
    "        # recall X is a string with all the punctuations etc removed\n",
    "        # We first convert all its words into their indices\n",
    "        X = self.text_to_integers(X)\n",
    "        # now convert this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the positive and negative reviews in each dataset have been stored in seperate folders and as such they have no explicit labels. We will therefore have to create our own labels as was done in [this](https://www.tensorflow.org/tutorials/load_data/text) official tensorflow tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return apply a label\n",
    "def labeler(review, label):\n",
    "    return review, tf.cast(label, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_paths = tf.data.Dataset.list_files(train_pos_path)\n",
    "train_pos_reviews = tf.data.TextLineDataset(train_pos_paths).map(lambda x : labeler(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Moonchild is a very difficult movie to categorise. It's easiest to think of it as several snapshots of the lives of the two central characters. The fact that these characters are members of a street gang set in an multicultural city of the near future and that one of them is a vampire does not preclude them from having moments like any other people, and this is one of the places where this movie is different to anything else I've ever heard of. It doesn't get wrapped up in the fact that one of the main characters is a vampire, it's just something that has to be dealt with like any other problem. The way the characters interact is surprisingly realistic- there are embarrassing relatives and tricks that are meant to look cool that just don't work, which leaves the film with a lovely sense of not taking itself too seriously for the most part.<br /><br />The other area that really stood out to me is the languages. The fictional city of Mallepa contains various cultural groups, and characters speak the language that they would be expected to speak. Japanese gang members speak Japanese to each other, but Chinese when talking to characters of Chinese descent. Possibly the most amusing exchange involves an Australian and is conducted in English. The actors of the four arguably main characters have three separate mother tongues between them and speak varying levels of each others' languages, so it's quite a feat that the movie was made at all. Which, I suppose, brings me to the lead actors.<br /><br />Much has been made of the fact that the movie stars two of Japan's biggest rockstars, Gackt and Hyde, as well as Taiwanese superstar Lee-hom Wang, whether it is to praise them for their acting or criticise it or simply fangirl about them. In my opinion, Lee-hom is the best at playing a straight and realistic character. However, any lack of acting ability on Gackt's part is mostly masked by the fact that the character he plays is prone to being over-dramatic. I wasn't sure if Hyde's character was supposed to be as sulky and sarcastic as he came across, but it doesn't really detract from the movie either way.<br /><br />There are several scenes which take rather melodramatic turns, which made it difficult for them to affect me much emotionally (Although this doesn't seem to stop a lot of people). I found it's best to just enjoy the movie for what it is and not take it too seriously- It's perfect for getting out and watching with a group of friends. It does have its flaws, but overall it was very enjoyable and I'd highly recommend it to anyone who doesn't mind a few subtitles.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Since I first saw this in the theater it has been my favorite. Since then I've seen it countless times and I never get tired of it. The setting has a lot to do with it (the Colorado I know would be jealous), but the storyline is original and I liked how it used small town mountain folk as the heroes. There has not been a movie I can compare this too. John Lithgow plays a smart villain, but I love how he is completely out of his element--he has to follow Tucker around and that's what keeps it interesting. This is an action movie at it's BEST. I don't think I'll see another that is so entertaining.<br /><br />You don't need 50,000 rounds fired to qualify as an action movie. It just has to keep you captivated, not shell-shocked.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_paths = tf.data.Dataset.list_files(train_neg_path)\n",
    "train_neg_reviews = tf.data.TextLineDataset(train_neg_paths).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Two days after seeing this thing, I'm still in agony over HAVING seen it. It's so bad, you have to wonder how anyone could write this tripe, much less allow it to be loose on the general public. Stilted acting, a leading man who looks like he's sleepwalking, and Alison Eastwood embarrassing herself. The action is indicative of low budget movie making, which means it is painfully bad. The plot? Well, if you were 6 years old, then you could have written this movie. Simplistic, idealistic, and just plain lame.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Generally I don't do minus's and if this site could i would give this movie -3 out of 10 meaning I really hated this movie. I thought Uwe Boll's alone in the dark was the worst i've seen yet but at least i gave it a 2.5 out of 10 in my opinion(Stephen Dorff shooting at nothing made me laugh so i boosted the ratings a bit). Hell if it was if compared to bloodrayne, Bloodrayne would win a Oscar for best movie if they were competing.<br /><br />Now to the plot, this movie is about the BTK killer which is fine but they've could have done better. The start looked OK but that's it I had to fast forward most of it because the death's where boring. I like killer movies and even if they suck they could still get some cool deaths. I'm not a fancy movie expert but believe me he would have shot himself if did see this. Sorry for rambling but there's nothing good to say about it, because it looks like someone took a camcorder and film this.. this.. thing of disaster. Uwe Boll your movies are no longer on my list of worst movies ever this took the cake.<br /><br />Well sorry i couldn't explain the plot(if there was one) but that was the best i could. Now if you don't mind i'm going to crawl into a corner and move back and forth and reminding me of how bad this movie scared me for life.... OK not for life\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_buffer_size = 20000\n",
    "train_dataset = train_pos_reviews.concatenate(train_neg_reviews).shuffle(buffer_size = shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in train_dataset:\n",
    "    count+=1\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to apply punc_filter_and_to_lower4() to the reviews in our training dataset. For this we will use the [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method of tf.data.Datasets. At this point it is important to note that (as mentioned in the documentation for [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)):\n",
    "\n",
    "    Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have two options:\n",
    "\n",
    "     1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.\n",
    "\n",
    "     2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is of concern to us because the [tokenizer](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) we used in punc_filter_and_to_lower4() only accepts string input and NOT tensors. This string was extracted from the tensor using its .numpy() method which is only operable in eager mode. Thus when we pass punc_filter_and_to_lower4() to map(), it throws an error, complaining:\n",
    "\n",
    "     'Tensor' object has no attribute 'numpy'\n",
    "\n",
    "\n",
    "Thus we have to wrap punc_filter_and_to_lower4() with tf.py_function before passing it to map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to wrap punc_filter_and_to_lower4 with tf.py_function\n",
    "# we will also use tf.ensure_shape(), or else tensorflow is \n",
    "# unable to statically determine it, leading to error being thrown during training\n",
    "def string_transform(X):\n",
    "    x = tf.py_function(punc_filter_and_to_lower4, [X], Tout = tf.string)\n",
    "    x = tf.ensure_shape(x, ())\n",
    "    \n",
    "    return x\n",
    "\n",
    "batch_size = 50\n",
    "prefetch = 2\n",
    "train_batch = train_dataset.map(lambda X, y: \n",
    "                                    (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have alternatively, preprocessed the data using punc_filter_and_to_lower2() which is based on tensorflow functions and then used it. The price we would have to pay is to rebuild the vocabulary accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train-batch: (50,)\n",
      "tf.Tensor(b'this is really a great unknown movie perfect dialogue without the typical clich\\xc3\\xa9s this movie relied on the actor s talent and it was pulled off it even had a little bit a comedy in it but it wasn t overdone once in the life is what a crime drama is supposed to be not the typical special affects garbage with sex thrown in i especially loved the interracial aspects of it all now onto the actors themselves laurence fishburne was superb at playing a career petty criminal it s a shame that he s only allowed to show his talent in his own movie titus welliver was fabulous as fishburne s junky half brother eamonn walker added flavor to the already perfectly spiced film paul calderon was perfect as a grease monkey drug lord i loved his acting since king of new york but the best acting in this film came from gregory hines and michael paul chan who were paired perfectly as two of calderon s henchmen once in the life is for sure a keeper 1 2 out of', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ite = next(iter(train_batch))\n",
    "print('shape of train-batch: {}'.format(ite[0].shape))\n",
    "print(ite[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       "array([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ite[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed in the excercise, we will create a validation dataset by splitting the test set into half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test\n",
      "path to positive test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/pos/*.txt\n",
      "path to negative test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/neg/*.txt\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(path, 'test')\n",
    "print(test_path)\n",
    "test_pos_path = os.path.join(test_path, 'pos', '*.txt')\n",
    "print('path to positive test reviews: \\n{}'.format(test_pos_path))\n",
    "test_neg_path = os.path.join(test_path, 'neg', '*.txt')\n",
    "print('path to negative test reviews: \\n{}'.format(test_neg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_files = glob.glob(test_pos_path)\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = 12500\n",
    "valid_pos_files = random.sample(test_pos_files, int(valid_size/2))\n",
    "len(valid_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can remove the valid_pos_files from the set of test_pos_files through the\n",
    "# simple trick explained in the following post:\n",
    "# https://stackoverflow.com/questions/6486450/python-compute-list-difference/6486467\n",
    "test_pos_files = list(set(test_pos_files) - set(valid_pos_files))\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial test_neg_files len:12500\n",
      "valid_neg_files len: 6250\n",
      "test_neg_files len: 6250\n"
     ]
    }
   ],
   "source": [
    "test_neg_files = glob.glob(test_neg_path)\n",
    "print('initial test_neg_files len:{}'.format(len(test_neg_files)))\n",
    "valid_neg_files = random.sample(test_neg_files, int(valid_size/2))\n",
    "print('valid_neg_files len: {}'.format(len(valid_neg_files)))\n",
    "test_neg_files = list(set(test_neg_files) - set(valid_neg_files))\n",
    "print('test_neg_files len: {}'.format(len(test_neg_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pos_reviews = tf.data.TextLineDataset(valid_pos_files).map(lambda x: labeler(x, 1))\n",
    "valid_neg_reviews = tf.data.TextLineDataset(valid_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I saw (unfortunately) the dubbed version on Encore.<br /><br />Student Paula Henning (Franka Potente who was also in the cult favorite \"Run Lola Run\") stars as a serious medical student who gets into a prestigious school in Germany. But she soon discovers that some students go missing and the bodies they work on in the anatomy lab are incredibly fresh...<br /><br />I was stuck seeing the dubbed version on Encore. It hurt a lot (the words not matching the lips got annoying real quick) but I still liked what I saw. The acting was good, it was beautifully photographed, it wasn\\'t TOO gruesome and I was never bored. Even more refreshing was a likable heroine who fights back when the bad guys go after her. The (mild) nudity was, in a refreshing twist, male! A previous poster mentions Benno Furmann (who is excellent) showed his butt but I don\\'t remember seeing it. Regardless this is a well done, scary and excellent thriller. From all I\\'ve read the original German language version is the best (I don\\'t doubt that for one second) but the dubbed version is watchable. I give this a 7.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Probably the best Royal Rumble in years.<br /><br />Match 1 sees Edge battle Shawn Michaels in a good but very long match. Next up one of the worst wrestlers on the roster - Heidenreich takes on The Undertaker in a boring casket match. Match number 3 sees Bradshaw defend his WWE title against Big Show and Kurt Angle in a surprisingly good contest. The next match up sees Triple H defending his 10th heavyweight title reign against Randy Orton in a great match up.<br /><br />Next up the Royal Rumble takes place in which 15 Raw superstars and 15 Smackdown! superstars hit the ring to try and win the rumble and face the champion whoever that may be at Wrestlemania 21. Highlights included Tough Enough 3 winner Daniel Puder getting his ass kicked by Chris Benoit, Eddie Guerrero and Hardcore Holly! All of the superstars beating the crap out of Muhammad Hassan, and Raw superstars vs Smackdown! superstars!<br /><br />Not a bad PPV at all.<br /><br />Edge vs HBK - 8/10 Taker vs Heidenreich - 4/10 JBL vs Angle vs Big Show - 7.5/10 Triple H vs Orton - 8.5/10 The Royal Rumble match - 9/10'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'First, I don\\'t see how the movie is on any \"best\" list or how it won any awards. Compared to La Pianiste, which is also on a \"best\" list, La Pianiste is gold. This movie lacked so many things, on so many different levels, but I can\\'t quite explain why I disliked it so much. The lead actor was annoying, I felt as though I never knew what was going on, and I was BORED!! Even though this was supposed to be some worthwhile life change that Pierre was starting, I wanted it to end.... as soon as possible. Why did it have to be his sister and cousin? Ugh. And why did Thibault get mean? He just bipolarly turned mean. And also, was it me or did I miss the whole purpose of what that guy in black was all about? Who were all those people playing music in the big basement of the big warehouse? Why did they have all that weird equipment and the guns and all those extra rooms for people to live in? I mean this in all seriousness, but does incest happen a lot in French culture? European culture? I took 5 years of learning about the culture and I never heard anything about that!'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Kokoda was inspired by events on the Kokoda track during WW2 when Australian militia slowed and ultimately stopped a push by 10,000 Japanese soldiers to move overland and capture Port Moresby. What they really mean is that the movie is set in this time period but is fiction and everything that happens is just a jumble of standard scenes from other war films. The first hour is just one clich\\xc3\\xa9 after another. Some of the scenes are simply there to be able to draw us into a feeling that this conflict was horrific beyond compare, when there appears to be little evidence of this. Both sides fought hard to control the track and no mercy was shown by either side. Both sides suffered from logistic shortages and the terrain was a great leveler in this conflict. As the Japanese got closer to Port Moresby their supply line grew and this ultimately led to their downfall. On the other hand as the Australians retreated closer to Port Moresby their supply line decreased. Some of the scenes appear to be straight out of the handbook on standard scenes to include in any war film. The film was misguided and highlighted the youth of the production team. At a time when Australia could have done with a great film about one of Australia's best moments the film Kokoda is a shallow disappointment.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_reviews = tf.data.TextLineDataset(test_pos_files).map(lambda x: labeler(x, 1))\n",
    "test_neg_reviews = tf.data.TextLineDataset(test_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I swear if I did ever tried cocaine I'd be able to relate to this film perfectly. Its pace, as well as the dialog, churns out at speeds that some viewers might need to stop and relax their heads.<br /><br />There are great little elements that pop up through out the film, like how Rob Lowe's character seems to always be loosing a shoe, or how some characters keep running spirals around his zigzagged path. The story was put together extremely well and the direction seems flawless.<br /><br />The movie reeks of clumsy and cuteness. This is one I think most could enjoy. A few laugh-out-loud-even-if-you-are-alone moments ensure that I'll certainly be watching this again.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'OK, the other reviewers have pretty much covered the main points of this great little gem, i.e. the story started out in life as material for Buster Keaton\\'s silent classic \"7 Chances\". Comedy, or acting in any genre for that much, is merely interpreting a scene and lines that someone else has written and performed before, if it\\'s not a totally original creation. Here we have The Stooges essentially doing material that was written and performed by someone else and yet for a low budget, short time span of a film, they\\'re handling things just fine. Regardless of what the credits say on their films, real \"stooge-philes\" know that they had a lot of input on lines and direction. They took their work as seriously as a surgeon does a vital operation. Words spoken by Emil Sitka himself during a documentary about the boys. Here, what appears to be their usual anarchy over something so simple as getting married, is actually organized chaos. Every line is perfectly timed with a related physical action. How many comedians are around today that can claim such mastery? Most obviously the Seinfeld crew but none others that I\\'ve seen in the last 35 years of watching TV. The critics will always \"pooh pooh\" The Stooges or Laurel & Hardy and others but then again...who ever remembers the critic\\'s names or what they said? Simply watch, laugh and enjoy!'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This movie is a fine example of what happens when a studio wants to get a sequel to a fine movie out of the gates at all cost. Only with this movie, it truly is a near miss. Everything seems in place for Robocop 2 to be a worthy followup to the groundbreaking first movie. The complete original cast (apart from the casualties, naturally) returns and gives it their best. Too bad a hackneyed script and an incompetent director as good as neutralize their efforts.<br /><br />Irvin Kershner might have been the ideal go to guy for George Lucas to direct the Empire Strikes Back. For a pedestrian filmmaker like Kershner there isn't much to ruin in Lucas' charmless film series. A worthy successor to a classic like Robocop would have needed either Paul Verhoeven to return, or a director with enough brass to give his own spin on it. Kershner doesn't know how to give his own spin on anything (Lucas hired him for that) and he's surely no Verhoeven.<br /><br />So what we get here is a movie that goes through all the motions to replicate the first movie, but with none of the freshness, humor or daring the original had. Kershner probably thought he could top Verhoeven by adding more gore and gratuitous violence, but instead he reveals how much he was at work as a director for hire instead of a passionate filmmaker. And that's a shame, since everything was in place to make this another classic. As mentioned the actors give it their best, but Phill Tippet delivers some groundbreaking stop motion effects and there are some great ideas in the story by Frank Miller, who was born to write a Robocop movie. If only the studio had hired a director who was competent enough to make all the potential come through.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I cannot BELIEVE anyone is giving this film a good rating. In addition to the terrible acting, thin (nonexistent?) plot line and slooooooooow pace, this would be the movie to watch if you were really TRYING to fall asleep. The writer\\'s and director\\'s brains must have been fried eggs to ever have concocted something as abominable as this. Based on the plot summary on the DVD case, the premise really sounded promising. But within the first ten minutes I knew it was a lost cause. If you want to see a REALLY creep take on the Area 51 idea, check out the remake of \"The Hills Have Eyes\". Dreamland will soon fade away as all pathetic films of its ilk do. NEXT!!!'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "valid_dataset = valid_pos_reviews.concatenate(valid_neg_reviews).shuffle(buffer_size = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch = valid_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_pos_reviews.concatenate(test_neg_reviews).shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing =test_dataset.map(lambda X,y: \n",
    "                                (text_to_integers(string_transform(X)), y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(52,), dtype=int64, numpy=\n",
       " array([52415, 74888, 13522, 34885, 52014, 25191, 21298, 36371, 41542,\n",
       "         3359, 29694, 46452, 10310,  7716,  4650, 38501,  7716, 74930,\n",
       "         7716, 48157,  6203, 13522, 11362,  7716, 74930, 30871,  2299,\n",
       "        17190, 43451, 47614,  7716, 55056, 27362, 31604, 66531,  9555,\n",
       "        10310,  7716, 74888, 13522, 62283, 50326, 59605, 46800, 48157,\n",
       "        31628, 46132, 33688, 17613, 40053, 27667, 67815])>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = next(iter(testing))\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
