{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create and train an embedding layer for the words appearing in a text data. We will then train a simple DNN based model to do sentiment analysis on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exercise 13.10 in [this](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "\n",
    "  - a. Download the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
    "  \n",
    "    \n",
    "  - b. Split the test set into a validation set (15,000) and a test set (10,000).\n",
    "  \n",
    "  \n",
    "  - c. Use tf.data to create an efficient dataset for each set.\n",
    "  \n",
    "  \n",
    "  - d. Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method.\n",
    "  \n",
    "  \n",
    "  - e. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n",
    "  \n",
    "  \n",
    "  - f. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.\n",
    "\n",
    "\n",
    "  - g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.1.0\n",
      "keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/prarit/MachineLearningProjects/Word-Embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('cwd: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good tutorial on using wget: https://www.tecmint.com/download-and-extract-tar-files-with-one-command/\n",
    "# turn off verbose output of wget using the flag -nv : https://shapeshed.com/unix-wget/#how-to-turn-off-verbose-output \n",
    "!wget -c -nv http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -o - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncompress the downloaded files\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tensorflow also provides this dataset: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md',\n",
       " '.gitignore',\n",
       " 'log_dir',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'aclImdb',\n",
       " 'Word-Embeddings.ipynb',\n",
       " 'aclImdb_v1.tar.gz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the files in the current working directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that aclImdb_v1.tar.gz was extracted to a folder called aclImdb. Let's see the contents of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of aclImdb are: \n",
      "['imdb.vocab', 'train', 'README', 'imdbEr.txt', 'test']\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd() , 'aclImdb')\n",
    "contents = os.listdir(path)\n",
    "print('The contents of aclImdb are: \\n{}'.format(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a README file in aclImdb, let us read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read README\n",
    "filepath = os.path.join(path, 'README')\n",
    "with open(filepath, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From README, we find that the train/test folder contains a 'pos' folder for positive reviews and a 'neg' for negative reviews along with .txt files containing urls of positive and negative reviews respectively. There are also some other files for bag-of-words features etc. \n",
    "\n",
    "Each train/test folder contains a total of 25000 reviews of which 12500 are positive reviews and 12500 are negative reviews.\n",
    "\n",
    "Let's verify the above about the 'train' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents of the train folder: \n",
      "['unsup', 'urls_pos.txt', 'pos', 'labeledBow.feat', 'unsupBow.feat', 'neg', 'urls_neg.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(path,'train')\n",
    "print('contents of the train folder: \\n{}'.format(os.listdir(train_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a dataset, we need the path to all the reviews. We can create the corresponding list of paths using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with positive reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the positive reviews in the training set\n",
    "train_pos_path = os.path.join(train_path, 'pos', '*.txt')\n",
    "train_pos_reviews = glob.glob(train_pos_path)\n",
    "print('No. of train-set files with positive reviews: {}'.format(len(train_pos_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the negative reviews in the training set\n",
    "train_neg_path = os.path.join(train_path, 'neg', '*.txt')\n",
    "train_neg_reviews = glob.glob(train_neg_path)\n",
    "print('No. of train-set files with negative reviews: {}'.format(len(train_neg_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us give a brief look at a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I watch them all.<br /><br />It's not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it's completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don't watch. 'How she move.' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It's just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I'm a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn't all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.\n"
     ]
    }
   ],
   "source": [
    "file = train_pos_reviews[0]\n",
    "with open(file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will like to load an preprocess all the data using tensorflow's data API, therefore let us quickly see how to read the same file as above but this time by using tensorflow's [TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'I watch them all.<br /><br />It\\'s not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it\\'s completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don\\'t watch. \\'How she move.\\' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It\\'s just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I\\'m a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn\\'t all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "pos_fl0 = tf.data.TextLineDataset(file)\n",
    "for item in pos_fl0:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As expected, we see that the pos_fl0 contains a single item and it's value output matches the output of the previous code cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having, learnt how to use tf.data.TextLineData() method, we can now starting preprocessing the data. In order to do this, we notice that the review contains punctuation marks and html line break tags etc. We will have to write a preprocessing function to get rid of these. Additionally, we will also change all alphabets to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing line-brk tags\n",
    "\n",
    "This can be very simply done by using the .replace() method of python strings. We can therefore use it to replace all occurrances of the line-break tag with a space. In tensorflow, the equivalent method is [tf.strings.regex_replace()](https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.strings.regex_replace\n",
    "Note that 'regex' in regex_replace() stands for [\"regular expression\"](https://docs.python.org/3/howto/regex.html). For e.g. the following will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('hello','e','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the following will throw an error: tf.strings.regex_replace('h(llo', '(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because \"(\" is a metacharacter. To match and replace meta-characters, we must prepend a backslash before them. This can be done as follows: '\\\\\\\\' + char. \n",
    "\n",
    "The previous code cell can now be made to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The error thrown by this code cell is intentional\n",
    "tf.strings.regex_replace('h(llo', '\\\\'+'(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h(llo','\\(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imp: \n",
    "\n",
    "Note that backslash itself is also a  meta-character. To search and replace backslash, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h\\llo', '\\\\'+'\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use '\\\\\\\\' and NOT '\\\\' to search and replace a backslash\n",
    "tf.strings.regex_replace('h\\llo', '\\\\\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try three different ways of removing punctuations from a tensorflow string and compare their timings:\n",
    "\n",
    "1) punc_filter_and_to_lower1: use tf.strings.unicode_decode() to convert all the characters in the string into an array of their ascii codes. We then loop through this array, skipping over the places where the entry matches the ascii code of a punctuation. Finally we call tf.strings.unicode_encode() on this array to convert the ascii codes back to characters, thereby obtaining a string with all the punctuations stripped. \n",
    "\n",
    "\n",
    "2) punc_filter_and_to_lower2: use tf.regex_replace() to search and replace each punctuation by empty space. Pay special attention to prepend backslash in order to be able to use meta-characters in regex_replace.\n",
    "\n",
    "\n",
    "3) punc_filter_and_to_lower3: simply extract the python string using its .numpy() method. Then simple iterate through the characters of the string, skipping over the punctutations. Join the resulting list of character using .join() method. \n",
    "\n",
    "4) punc_filter_and_to_lower4: Use [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) by setting it's \"alphanum_only\" arg to True. This way it will only parse alpha-numeric characters in the text and to split the text at occurrances of non-alphanumeric characters. Since whitespace is a non-alpha-numeric character, the output will largely consist of a list of words in the text.  The caeat with this approach is that words with apostrophe in them such as \" don't \" will be split into two words: \"don\" and \"t\". Also, [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) does NOT consider underscores as non-alpha-numeric, so underscores do not get removed form the text. On the other hand since, the tokenizer already generates a list of words, we use this list to generate a vocobulary of words in the dataset at this step it-self, making the preprocessing faster.\n",
    "\n",
    "There timing on the first review in the training set was as follows:\n",
    "\n",
    "1) punc_filter_and_to_lower1: Wall time: ~ 3 s\n",
    "\n",
    "2) punc_filter_and_to_lower2: Wall time: ~ 8 ms\n",
    "\n",
    "3) punc_filter_and_to_lower3: Wall time: ~ 6 ms\n",
    "\n",
    "4) punc_filter_and_to_lower4: Wall time: ~ 6 ms (when not udating a vocabulary of words) \n",
    "\n",
    "5) 4) punc_filter_and_to_lower4: Wall time: ~ 7 ms (when also udating a vocabulary of words) \n",
    "\n",
    "Clearly, the last function is the fastest with the first one being extremely slow (takes several secs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations before utf encoding: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "punctuations after utf encoding: \n",
      "[ 33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  58  59  60\n",
      "  61  62  63  64  91  92  93  94  95  96 123 124 125 126]\n"
     ]
    }
   ],
   "source": [
    "# list of punctuations\n",
    "punc_ls = string.punctuation\n",
    "print('punctuations before utf encoding: {}'.format(punc_ls))\n",
    "punc_ls2 = tf.strings.unicode_decode(punc_ls, input_encoding = 'UTF-8')\n",
    "print('punctuations after utf encoding: \\n{}'.format(punc_ls2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.unicode_encode(), tf.strings.unicode_decode()\n",
    "def punc_filter_and_to_lower1(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = tf.strings.unicode_decode(st2, input_encoding = 'utf-8')\n",
    "    st2 = tf.strings.unicode_encode([char for char in st2 if char not in punc_ls2], output_encoding = 'UTF-8')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.38 s, sys: 60.7 ms, total: 3.44 s\n",
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower1(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.regex_replace()\n",
    "def punc_filter_and_to_lower2(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ')\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    for punc in punc_ls:\n",
    "        # V.imp: to replace meta-characters we prepend a backslash to them \n",
    "        # infact we can also prepend a backslach before all the characters\n",
    "        # to prepend a backslash before a character we do: '\\\\' + char\n",
    "        st2 = tf.strings.regex_replace(st2, '\\\\'+ punc, ' ')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.48 ms, sys: 0 ns, total: 7.48 ms\n",
      "Wall time: 6.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower2(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all  it s not better than the amazing ones   strictly ballroom    shall we dance    japanese version   but it s completely respectable and pleasingly different in parts  i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting  for example  the  name should scream don t watch   how she move   since when can movie titles ignore grammar    there is nothing inherently incorrect about caribbean english grammar  it s just not canadian standard english grammar  comments about the dialogue seem off to me  i put on the subtitles because i m a canadian standard english speaker  so i just automatically assumed that i would have trouble understanding all of it  it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american  i loved that this movie was set in toronto and  in fact  wish it was even more clearly set there  i loved that the heroine was so atypically cast  i enjoyed the stepping routines  i liked the driven mum character  i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies  in summary  if you tend to like dance movies  then this is a decent one  if you have superiority issues about the grammar of the english standard you grew up speaking  your narrow mind may have difficulty enjoying this movie '>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using .join() method in python string class\n",
    "def punc_filter_and_to_lower3(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = st2.numpy().decode('utf-8')\n",
    "    st2 = ''.join([char for char in st2 if char not in punc_ls])\n",
    "    \n",
    "    st2 = st2.lower()\n",
    "    \n",
    "    st2 = tf.convert_to_tensor(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 ms, sys: 3.54 ms, total: 5.28 ms\n",
      "Wall time: 4.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower3(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tfds.features.text.tokenizer()\n",
    "# we will also simultaneously generate a vocabulary in this step\n",
    "import tensorflow_datasets as tfds\n",
    "def punc_filter_and_to_lower4(st, vocab = None):\n",
    "    ''' vocab: vocabulary to update with the words in st'''\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.lower(tf.strings.regex_replace(st, line_brk_tag, ' '))\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    tokenizer = tfds.features.text.Tokenizer()\n",
    "    words = tokenizer.tokenize(st2.numpy()) # note that the inpout has to be a python string NOT as tensor \n",
    "                                            # The output is a list NOT a tensor\n",
    "                                            # https://stackoverflow.com/questions/56665868/tensor-numpy-not-working-in-tensorflow-data-dataset-throws-the-error-attribu\n",
    "    st2 = tf.strings.join(words, ' ')\n",
    "    \n",
    "    if type(vocab) == set:\n",
    "        vocab.update(words)\n",
    "        return st2, vocab\n",
    "    \n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.23 ms, sys: 667 µs, total: 7.9 ms\n",
      "Wall time: 7.03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower4(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all it s not better than the amazing ones _strictly ballroom_ _shall we dance _ japanese version but it s completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream don t watch how she move since when can movie titles ignore grammar there is nothing inherently incorrect about caribbean english grammar it s just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because i m a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.9 ms, sys: 0 ns, total: 7.9 ms\n",
      "Wall time: 7.35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test the timing of punc_filter_and_to_lower4 when simultaneously updating a dictionary\n",
    "vocab2 = set([])\n",
    "for item in pos_fl0:\n",
    "    lst, vocab2 = punc_filter_and_to_lower4(item, vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary based on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be easily done using python's [set()](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset) container.: basically, split each text in the training instance into words and update the set with this list. This will add any new words in the text to the set. In the end, the set will contain all the unique words in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have also implemented the above idea in our function punc_filter_and_to_lower4() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a vocabulary using the set() container\n",
    "vocab1 = set([]) # python set for containing unique words in the training dataset\n",
    "def vocab_builder1(strng):\n",
    "    # split the string into its words\n",
    "    words = tf.strings.split(strng)\n",
    "    # update vocab\n",
    "    vocab1.update(words.numpy())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 223 ms, sys: 87.9 ms, total: 311 ms\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower3(item)\n",
    "    vocab_builder1(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocab_builder1() function as defined above takes about 300 ms. On the otherhand, in punc_filter_and_to_lower4() function,  we were able to get almost the same result (upto the caveats mentioned in the previous section) by using tfds.features.text.Tokenizer(), in about 8 ms. Clearly, there is a difference of few order of magnitude between the time taken by the two routines with the difference between their output being quite tolerable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================="
     ]
    }
   ],
   "source": [
    "vocab = set([])\n",
    "train_filepaths = tf.data.Dataset.list_files([train_pos_path, train_neg_path])\n",
    "train_dataset = tf.data.TextLineDataset(train_filepaths)\n",
    "\n",
    "ctr = 0\n",
    "for item in train_dataset:\n",
    "    _ , vocab = punc_filter_and_to_lower4(item, vocab)\n",
    "    \n",
    "    ctr+=1\n",
    "    if ctr%255 ==0:\n",
    "        print(\"=\", end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['signifying', 'jacoby', 'cb', 'bo', 'haviland']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick 5 random words from vocab to inspect it\n",
    "import random\n",
    "\n",
    "random.sample(vocab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words in vocabulary: 74893\n"
     ]
    }
   ],
   "source": [
    "print('no. of words in vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a lookup table based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "lookup_initializer = tf.lookup.KeyValueTensorInitializer( list(vocab), indices) \n",
    "num_oov_buckets = 50 \n",
    "lookup_table = tf.lookup.StaticVocabularyTable( initializer = lookup_initializer, \n",
    "                                               num_oov_buckets = num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map a text to integers using the above lookup table\n",
    "def text_to_integers(X):\n",
    "    X = tf.strings.split(X)\n",
    "    int_ls = lookup_table.lookup(X)\n",
    "    return int_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[46350  3059  5078 20335 28188 72063 28942 68439 26061 64912 17677 31130\n",
      " 34716 49605 31514 64030 40400  2445  5248 40021 22514 28188 72063 64123\n",
      "  7194 59628 22442 41731 17542 16607 46350 66203 36159  8081 41819 59628\n",
      " 46350 45252 23142  5785 64912 56137 26356 53457 17542 23142  5785 69651\n",
      " 55397 40451 35030 32811 20479 64912 70746 13161 29615 17835 18146  3059\n",
      " 50232 32506 34089 64228  3998  6933 55026 49740 30863 18365 44533  1941\n",
      " 53205 72700 60409 26356 31257  8081 18365 28188 72063 35345 28942 25686\n",
      "  2350  8081 18365 11123 26356 64912 66697  3496 53178  5571 11084 46350\n",
      " 25693 35351 64912 60598 39953 46350 34723 17863 25686  2350  8081 30628\n",
      "   648 46350 35345 54695 62393 56239 46350 12051 21251 63413 43828 20335\n",
      "  5785 28188 28188 30728 18146 20335 56239 57003 59628 28188 19473 17863\n",
      " 19724 41731  4255 65998 64912 12717 56529 44724 46350 21251 47997 26017\n",
      "   648 66654 46350 38928 56239 35404 55026 30421 52609 17542 15417 59628\n",
      " 17542 42201 11503 28188 30421 47948  7775 73203 52609 44533 46350 38928\n",
      " 56239 64912 23644 30421   648 44478 28426 46350 39741 64912 10370 48888\n",
      " 46350 61300 64912 19358 69152  9864 46350 73351 56239 39995  5785 64912\n",
      " 64497 17542 64912 55026 26017 17815  7775  3489 26061  1941 52810  5785\n",
      " 40400 44724 17542 38130 63661 19265 37395  5571 61137 40400 44724 12382\n",
      " 35404  1941 17863 67822 53167 63661 19265 21251 47628 64497 26356 64912\n",
      " 18365  5785 64912  8081  2350 19265 12513 55035 58271 56373 19746  3578\n",
      " 40123 21251 26935 21623 35404 55026], shape=(246,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(74943, 100) dtype=float32, numpy=\n",
       "array([[0.34155226, 0.32195544, 0.06046116, ..., 0.7618345 , 0.23363006,\n",
       "        0.7218468 ],\n",
       "       [0.94264495, 0.5660316 , 0.5678663 , ..., 0.6864557 , 0.8130299 ,\n",
       "        0.45111203],\n",
       "       [0.5040668 , 0.6374707 , 0.40425622, ..., 0.24970484, 0.70195913,\n",
       "        0.6720092 ],\n",
       "       ...,\n",
       "       [0.2695012 , 0.9302741 , 0.48979366, ..., 0.30459356, 0.45468497,\n",
       "        0.5634079 ],\n",
       "       [0.71090305, 0.51647925, 0.11278117, ..., 0.08980632, 0.5763416 ,\n",
       "        0.63185406],\n",
       "       [0.7893827 , 0.17041099, 0.12543333, ..., 0.87857676, 0.89390683,\n",
       "        0.39723885]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "initial_embeddings = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim] ) \n",
    "embedding_matrix = tf.Variable(initial_embeddings)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an embedding-vector from a list of word indices\n",
    "def indices_to_embeddings(idxs):\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix, idxs)\n",
    "    sqrt_num_words = tf.math.sqrt(tf.cast(tf.size(idxs), tf.float32)) # size is of type int32 but \n",
    "                                                                      # tf.math.sqrt has to have one of \n",
    "                                                                      # bfloat16, half, float32, \n",
    "                                                                      # float64, complex64, \n",
    "                                                                      # complex128.\n",
    "    mean_embedding = tf.math.reduce_mean(embeddings, axis = 0)\n",
    "    sentence_embedding = tf.multiply(sqrt_num_words, mean_embedding)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    item = indices_to_embeddings(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([8.021399 , 7.5330133, 8.903438 , 8.36042  , 8.680352 , 7.7793856,\n",
       "       7.8757954, 6.9797244, 8.2482605, 8.485479 , 7.8188663, 7.7725515,\n",
       "       8.077141 , 8.468325 , 7.9781365, 7.7117767, 7.358423 , 7.2856717,\n",
       "       7.887798 , 8.400873 , 7.630314 , 7.3915505, 8.077708 , 7.1228642,\n",
       "       8.002858 , 6.9399304, 6.977312 , 7.184468 , 7.930327 , 8.901015 ,\n",
       "       6.593821 , 7.7483487, 6.6298494, 7.3742657, 8.193884 , 8.034278 ,\n",
       "       7.900299 , 6.926827 , 7.293513 , 7.1158457, 7.803854 , 8.411332 ,\n",
       "       8.030312 , 7.1683154, 7.826986 , 7.158508 , 8.503684 , 8.102309 ,\n",
       "       7.8942018, 8.811254 , 7.4442363, 6.400973 , 7.752815 , 8.409023 ,\n",
       "       7.9036746, 7.4893737, 7.1362505, 7.2599525, 6.9212685, 8.290645 ,\n",
       "       7.4075074, 7.637827 , 8.184101 , 8.2632265, 8.085599 , 7.1866646,\n",
       "       7.765825 , 8.545707 , 8.134727 , 8.436356 , 8.424783 , 8.005594 ,\n",
       "       7.7233806, 7.7250967, 7.942512 , 6.993189 , 8.623959 , 8.444454 ,\n",
       "       7.01823  , 8.523306 , 7.20158  , 7.4579506, 7.924753 , 8.089464 ,\n",
       "       7.171895 , 8.407579 , 8.4453535, 7.235266 , 7.598843 , 7.9352264,\n",
       "       7.0544925, 8.743309 , 7.4420347, 8.181737 , 8.457189 , 8.003212 ,\n",
       "       8.984337 , 7.710651 , 7.8422656, 7.5096545], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessing layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a preprocessing layer that accepts a sentence as an input, removes punctuations and line-brk tags form it, then converts it's words into indices and finally output an embedding vector corresponding to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_layer(keras.layers.Layer): \n",
    "    def __init__(self, vocab,  num_oov_buckets, embedding_len, kernel_regularizer = None, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab = list(vocab)\n",
    "        self.vocab_len  = tf.size(self.vocab, out_type = tf.int64) \n",
    "        self.num_oov_buckets = num_oov_buckets\n",
    "        self.embedding_len = embedding_len\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.indices = tf.range(self.vocab_len, dtype = tf.int64)\n",
    "        self.lookup_initializer = tf.lookup.KeyValueTensorInitializer(self.vocab, \n",
    "                                                                      self.indices)\n",
    "        self.lookup_table = tf.lookup.StaticVocabularyTable(initializer = self.lookup_initializer, \n",
    "                                                            num_oov_buckets = self.num_oov_buckets)\n",
    "         \n",
    "    # build the embedding matrix \n",
    "    def build(self, batch_input_shape):\n",
    "        self.embedding_mat = self.add_weight(name = 'embedding matrix', \n",
    "                                             shape = [self.vocab_len + self.num_oov_buckets, self.embedding_len],\n",
    "                                             initializer = tf.random_uniform_initializer)\n",
    "        \n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    \n",
    "    # function to convert a sentence into a list of indices for its words\n",
    "    def text_to_integers(self, X):\n",
    "        X_words = tf.strings.split(X)\n",
    "        # X_words will be a ragged tensor\n",
    "        # on the otherhand, self.lookup_table.lookup() expects either a Sparse tensor or a dense tensor\n",
    "        # for example see the documentation: https://www.tensorflow.org/api_docs/python/tf/lookup/StaticVocabularyTable\n",
    "        # in order to use it ragged tensors, we have to use tf.ragged.map_flay_values()\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/ragged/map_flat_values\n",
    "        X_int = tf.ragged.map_flat_values(self.lookup_table.lookup, X_words)\n",
    "        return X_int\n",
    "    \n",
    "    # function to convert a list of indices into an embedding vector\n",
    "    def integers_to_embeddings(self, X):\n",
    "        sqrt_num_words = tf.linalg.diag(\n",
    "            tf.math.sqrt(tf.cast(X.nested_row_lengths(), tf.float32))[0])\n",
    "        X_ems = tf.nn.embedding_lookup(self.embedding_mat, X)\n",
    "        X_vec = tf.math.reduce_mean(X_ems, axis = 1) # note that X_ems is a ragged tensor whose \n",
    "                                                     # 0-th dim. corresponds to the batch dim. \n",
    "                                                     # therefore axis = 1 and NOT 0 when taking mean\n",
    "        \n",
    "        return sqrt_num_words@X_vec\n",
    "        \n",
    "    # define the call method\n",
    "    def call(self, X):\n",
    "        \n",
    "        # recall X contains strings with all the punctuations etc removed\n",
    "        # Imp: The shape of X is (batch_size, ...) i.e. the 0-th axis of X corresponds to batch dim. \n",
    "        # We first convert all its words into their indices\n",
    "        X_int = self.text_to_integers(X)\n",
    "        # now convert this a mean embedding vector\n",
    "        X_vec = self.integers_to_embeddings(X_int)\n",
    "        \n",
    "        if self.kernel_regularizer:\n",
    "            self.add_loss(self.kernel_regularizer(self.embedding_mat))\n",
    "        \n",
    "        return X_vec\n",
    "    \n",
    "    \n",
    "    # compute output shape\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.embedding_len])\n",
    "    \n",
    "    \n",
    "    # add the layers hyperparameters to the configuration file\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'embedding_len': self.embedding_len, \n",
    "                'num_oov_buckets': self.num_oov_buckets}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the positive and negative reviews in each dataset have been stored in seperate folders and as such they have no explicit labels. We will therefore have to create our own labels as was done in [this](https://www.tensorflow.org/tutorials/load_data/text) official tensorflow tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return apply a label\n",
    "def labeler(review, label):\n",
    "    return review, tf.cast(label, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_paths = tf.data.Dataset.list_files(train_pos_path)\n",
    "train_pos_reviews = tf.data.TextLineDataset(train_pos_paths).map(lambda x : labeler(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'When I watched this film the first time, it was a taped copy and the title was/is Caged Terror. I still own the tape, and I confess, I\\'ve watched it more than once from beginning to end! The film is extremely low budget and the dialogue is often unintentionally amusing! I have gotten a few of my friends to watch this and we\\'ve had some great laughs from the terrible script. The film concerns a couple, (remember this is like early 70\\'s so they are just too hip man!) who go on a week-end camping trip in what I believe was supposed to be upstate NY. They have some hilarious dialogue after catching and eating a fish and the girl bemoans the death of the fish and that they ate it! The guy comes back with something goofy about how they ate the fish and now it was a part of them, and he goes; \"And that\\'s beautiful man!\" Heavy man, really heavy! LOL! Anyway, along come a couple of Vietnam vets, one of who plays the flute, I believe. (At any rate they are musical fellows!) The guys are clearly attracted to the girl and when the couple prove unfriendly, they end up terrorizing them during the night. The guy ends up caged in a chicken coop, and has to watch his girl friend being ravished by the two guys. Actually, by the end of the night, she seems to be pretty into it, and when morning comes, the guys leave and the girl and guy are free to leave. Supposedly the guy has learned a lesson about how to treat people, and the girl has a smile on her face! :) Anyway, I would recommend this film highly to anyone looking for a damn good laugh! It never fails to amuse me anyway! If I could find this on DVD and replace my old tape copy, I\\'d actually buy it again, it\\'s classic camp! You gotta love this stuff!'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I\\'ve really enjoyed this adaptation of \"Emma\".I have seen it many times and am always looking forward to seeing it again.Though it only lasts 107 minutes, most of the novel plot and sub-plots were developed in a satisfactory way. All the characters are well-portrayed. Most of the dialogues come directly from the novel with no silly jokes added as in Emma Thompson\\'s Sense and Sensibility.<br /><br />As a foreigner, I particularly appreciate the perfect diction of the actors. The setting and costumes were beautiful. I find this version quite on a par with the 1995 miniseries \"Pride and Prejudice\" but then the producer and screenwriter were the same. Kate Beckinsale did a really good job portraying \"Emma\" of whom Jane Austen said she would create a heroin no-one but her would love. She is snobbish but has just enough youth and inexperience to be still likable. Mark Strong was also very good at portraying Mr Knightley, not an easy part, I think, though he has not the charisma shown by Colin Firth\\'s Mr Darcy in Pride and Prejudice. Even the end scene (the harvest festival) which does not happen in the novel provides a fitting end except for when it shows Emma being cold and almost unpleasant with Frank Churchill whereas in the novel she was thoroughly reconciled with him, even telling him that she would have enjoyed the duplicity, had she been in his situation. A strange departure from the faithfulness otherwise shown throughout the film. I find the costumes more beautiful and elaborate than in other adaptations from Jane Austen\\'s novels.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_paths = tf.data.Dataset.list_files(train_neg_path)\n",
    "train_neg_reviews = tf.data.TextLineDataset(train_neg_paths).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Laughed a lot - because it is so incredibly bad - sorry folks, but definitely one of the worst movies I have ever seen... I know it is low budget, but anyway: the actors behave like playing in a soap, the dialogues are absolutely crappy and the last time I have seen such odd pictures was at a trash nite at some youth video festival ten years ago. I really appreciate that people gather together and shoot cheap movies, but at least a certain amount of quality should be accomplished. But at least one good thing: the first three minutes of the movie were quiet interesting and looked okay - and the score was really worth listening to. The DVD cover promised a lot, but that is by far the best this film has to offer...'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Being that I loved the original \"Caligula\" even with all its flaws, I have to say this remake trailer was abysmal.<br /><br />Listening to Jovovich say in that lazy American accent \"Mmm cuhligyooluh...\" makes me feel sick. The set doesn\\'t look Roman at all... it looks like some rich actor\\'s Hollywood mansion backyard, and the Roman costumes look like cheap crap you buy at a suburban costume shop.<br /><br />That \"charming\" Adriana Asti looks like a fifty year old Hispanic woman totally terrified out of her mind, as if not knowing it\\'s a movie trailer.<br /><br />The acting has got to be some of the worst I\\'ve ever seen, with most of the lines I hear being random actors screaming \"CALIGULAAAAAAAAAAAAAAAAA!!!!!!!!\" as loudly and obnoxiously annoyingly as they can.<br /><br />The random sex scenes also filled a good 40 or 50% of the trailer, and the scenes with notable actors/actresses like Gerard Butler (who graces the screen in shadows for all of three and a half seconds) not doing anything but looking uncomfortable or going all-out over the top with their minimal lines, just dragging it down with the ridiculousness of their delivery.<br /><br />Courtney Love\\'s part consists of her looking like her usual dumpy crack-whore self leaning against a door mumbling about the moon or something. You can\\'t tell because she\\'s either drunk or high or just mumbling idiotically.<br /><br />Karen Black is just annoying... randomly laughing, and screaming in such a way that irritates you.<br /><br />Helen Mirren... she was in the original, and her return to \"Caligula\" consists of... \"CAAAAAAAAAAAAAAAAAAAAAALIGULAAAAAAAAAAAAAAAAAAAAAAA!!!!!!!!!!!!!!\" I particularly \"love\" (and by love, I mean hate) Ennia\\'s line, \"Caligola... j00 no maki me *something incoherent*.... *something that sounds like j00nalo*\" It\\'s also insanely arrogant to say Caligula\\'s four year reign was greater than Jesus\\'s birth and death.<br /><br />Honestly, this has got to be the worst, most exploitative, self-indulgently arrogant piece of crap labeled as \"art\" I\\'ve ever seen. Even if Gore Vidal hated the original Caligula, he shouldn\\'t have shown up or given his name over for this crap-pile, no matter how much they paid him (unless it was a billion trillion yen). Worst trailer ever.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_buffer_size = 20000\n",
    "train_dataset = train_pos_reviews.concatenate(train_neg_reviews).shuffle(buffer_size = shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in train_dataset:\n",
    "    count+=1\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to apply punc_filter_and_to_lower4() to the reviews in our training dataset. For this we will use the [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method of tf.data.Datasets. At this point it is important to note that (as mentioned in the documentation for [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)):\n",
    "\n",
    "    Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have two options:\n",
    "\n",
    "     1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.\n",
    "\n",
    "     2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is of concern to us because the [tokenizer](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) we used in punc_filter_and_to_lower4() only accepts string input and NOT tensors. This string was extracted from the tensor using its .numpy() method which is only operable in eager mode. Thus when we pass punc_filter_and_to_lower4() to map(), it throws an error, complaining:\n",
    "\n",
    "     'Tensor' object has no attribute 'numpy'\n",
    "\n",
    "\n",
    "Thus we have to wrap punc_filter_and_to_lower4() with tf.py_function before passing it to map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to wrap punc_filter_and_to_lower4 with tf.py_function\n",
    "# we will also use tf.ensure_shape(), or else tensorflow is \n",
    "# unable to statically determine it, leading to error being thrown during training\n",
    "def string_transform(X):\n",
    "    x = tf.py_function(punc_filter_and_to_lower4, [X], Tout = tf.string)\n",
    "    x = tf.ensure_shape(x, ())\n",
    "    \n",
    "    return x\n",
    "\n",
    "batch_size = 50\n",
    "prefetch = 2\n",
    "train_batch = train_dataset.map(lambda X, y: \n",
    "                                    (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have alternatively, preprocessed the data using punc_filter_and_to_lower2() which is based on tensorflow functions and then used it. The price we would have to pay is to rebuild the vocabulary accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train-batch: (50,)\n",
      "tf.Tensor(b'if you like mech war games it s pretty good some of it is cheap but the robot fights is worth seeing i ve enjoyed the mech war field for some time and this is pretty much the only movie i ve ever seen that come close to that feeling of what it would be like to pilot one of those huge mechs if you like the genera then games you like are mech warrior three and four and if you have an xbox and 350 to spare steel battalion the movie is worth seeing at least once there really needs to be some more movies on the same theme out there less remakes and more original works enjoy', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ite = next(iter(train_batch))\n",
    "print('shape of train-batch: {}'.format(ite[0].shape))\n",
    "print(ite[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       "array([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ite[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed in the excercise, we will create a validation dataset by splitting the test set into half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test\n",
      "path to positive test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/pos/*.txt\n",
      "path to negative test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/neg/*.txt\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(path, 'test')\n",
    "print(test_path)\n",
    "test_pos_path = os.path.join(test_path, 'pos', '*.txt')\n",
    "print('path to positive test reviews: \\n{}'.format(test_pos_path))\n",
    "test_neg_path = os.path.join(test_path, 'neg', '*.txt')\n",
    "print('path to negative test reviews: \\n{}'.format(test_neg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_files = glob.glob(test_pos_path)\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = 12500\n",
    "valid_pos_files = random.sample(test_pos_files, int(valid_size/2))\n",
    "len(valid_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can remove the valid_pos_files from the set of test_pos_files through the\n",
    "# simple trick explained in the following post:\n",
    "# https://stackoverflow.com/questions/6486450/python-compute-list-difference/6486467\n",
    "test_pos_files = list(set(test_pos_files) - set(valid_pos_files))\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial test_neg_files len:12500\n",
      "valid_neg_files len: 6250\n",
      "test_neg_files len: 6250\n"
     ]
    }
   ],
   "source": [
    "test_neg_files = glob.glob(test_neg_path)\n",
    "print('initial test_neg_files len:{}'.format(len(test_neg_files)))\n",
    "valid_neg_files = random.sample(test_neg_files, int(valid_size/2))\n",
    "print('valid_neg_files len: {}'.format(len(valid_neg_files)))\n",
    "test_neg_files = list(set(test_neg_files) - set(valid_neg_files))\n",
    "print('test_neg_files len: {}'.format(len(test_neg_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pos_reviews = tf.data.TextLineDataset(valid_pos_files).map(lambda x: labeler(x, 1))\n",
    "valid_neg_reviews = tf.data.TextLineDataset(valid_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"In retrospect, the 1970s was a golden era for the American cinema, as demonstrated and explored by this documentary directed by Ted Demme and Richard LaGravenese. This IFC effort serves to illustrate and clarify the main idea of what that time meant for the careers of these illustrious people seen in the documentary.<br /><br />The amazing body of work that remains, is a legacy to all the people involved in the art of making movies in that period. The decade was marked by the end of the Viet Nam war and the turbulent finale of those years of Jimmy Carter's presidency.<br /><br />One thing comes out clear, films today don't measure against the movies that came out during that creative decade because the industry, as a whole, has changed dramatically. The big studios nowadays want to go to tame pictures that will be instant hits without any consideration to content, or integrity, as long as the bottom line shows millions of dollars in revenues.<br /><br />The other thing that emerges after hearing some of America's best creative minds speak, is the importance of the independent film spirit because it is about the only thing that afford its creators great moral and artistic rewards.<br /><br />This documentary is a must see for all movie fans.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I did enjoy watching Squire Trelane jerk around the crew in this episode, though after a while the whole thing just seemed a little too long. Sure, the histrionics were kind of funny for a while, and the ending was a pretty good way to wrap the whole thing together. I think the problem was that I enjoyed seeing Trelane when he was full of bravado and fun, the fun seemed to vanish when Trelane became vindictive and nasty. Talk about a mood killer--going from the obnoxious but affable host to the guy sentencing Kirk to death! But, despite this, the episode was enjoyable and worth my time. For die-hard Trekkies, this is a must-see, for others it's just a pretty run of the mill one.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I\\'m not really much of an Abbott & Costello fan (although I do enjoy \"Who\\'s On First\") and, to be honest, there wasn\\'t much in this movie that would inspire me to watch any more of their work. It wasn\\'t really bad. It had some mildly amusing scenes, and actually a very convincing giant played by Buddy Baer, but somehow, given the fame of the duo and the esteem in which they\\'re generally held, I have to say I was expecting more. As the story goes, the pair stumble into a babysitting job, and during the reading of Jack & The Beanstalk as a bedtime story (with the kid reading it to Costello), Costello\\'s Jack falls asleep and dreams himself into the story. There\\'s a \"Wizard Of Oz\" kind of feel to the story, in that the characters in the dream are all the equivalents of real-life acquaintances of Jack, and the movie opens in black & white and shifts to colour during the dream sequence. The fight scenes between Jack and the giant and the dance scene between Jack and Polly (Dorothy Ford) are among the amusing parts of the movie. Polly, of course, also leads to one of the questions of the movie - what happened to her? Jack and gang apparently left her behind in the giant\\'s castle! I know - it was just a dream, so who cares. Still - I wondered. There were also a couple of cute song and dance routines. My 4 year old giggled a bit during this, so she was able to appreciate some of the humour. I found it to be an acceptable timewaster, but certainly not anything that would convince you of Abbott and Costello as comic geniuses. 4/10'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"A group of tourists are stranded on Snake Island after an unfortunate accident with their boat. They are forced to spend the night and as you probably suspected, it isn't called Snake Island because it's just soooooo much fun to say - it has a history of people disappearing one by one because of the large snake population, which is just what happens with these poor dumb souls. This is a very boring and typical movie with tons of off screen snake attacks and lousy performances from NOBODY actors. The only somewhat entertaining scene was an absolutely unnecessary and forced strip scene which ain't anything couldn't see in a PG13 rated movie, folks. If you are into snake movies than check out SSSSSSS, but don't torture yourself with this crap.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_reviews = tf.data.TextLineDataset(test_pos_files).map(lambda x: labeler(x, 1))\n",
    "test_neg_reviews = tf.data.TextLineDataset(test_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I saw this stage show when it was broadcast on PBS in 1983. I was involved in local theatre at the time and had seen some pretty incredible stuff out of the Dell Arte Players, but Bill Irwin floored me.<br /><br />I was most impressed at how a man of his size (he\\'s quite tall and beefy) could fold himself up into a small box without so much as a pause for adjustment and move across the stage at a dead run without even a whisper of sound from his feet if he chose not to make any noise.<br /><br />Most amazing for me, though, in this performance, was the way he rose to his feet during the jack-in-the-box / marionette piece. Those who saw this show will recall that when he climbed from the box and collapsed to the floor with his body limp and limbs akimbo, he \"pulled\" himself up by the top of his head as if by a string, and rose not just to his feet, but to a full ballet point\\xc2\\x97and did it in one fluid, seemingly effortless motion. Just consider the strength, grace, balance and focus such a series of movements must take in order to accomplish them the way he did! Add to his physical prowess his strong and believable characterization skills, and there lies a consummate actor / performer. My jaw dropped at the movement and my heart broke at the portrayal of a puppet who is determined to be more than just a lifeless thing in a box.<br /><br />As to the unfortunate (yes\\xc2\\x97\"tragic\" would be a better word) unavailability of this piece in home media form, I have noticed that much of PBS\\' works are not available on tape or DVD. Sometimes, PBS shows will be available for direct purchase from them for a limited time immediately following a broadcast, but they seldom stay on the market for long. There are exceptions, of course, but these are mainly the science and history documentaries; rarely does an arts piece remain in print for long\\xc2\\x97assuming it ever made it into VHS / DVD to begin with. I don\\'t know why this should be so; certainly, PBS could use the income from home media marketing of their shows, but they don\\'t take advantage of it much. This is a shame. There are many things I\\'ve watched on PBS that I wish to own, but pieces such as \"The Regard of Flight\" are, I\\'m afraid, a one-shot, once in a lifetime treat, never to be repeated on PBS again and never to be available for home media purchase. That really sucks. I\\'m lucky to have caught it when I did.<br /><br />Oh, yeah\\xc2\\x97our local library did get a copy of \"The Regard of Flight.\" And yes\\xc2\\x97it was stolen.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I saw this film at the 2002 Toronto International Film Festival.<br /><br />This is the first Indian film I've seen in the Tamil language, and while it does share some similarities with other Indian films (wonderful music and choreography, sweeping storyline), the director attempts more than just to entertain. The film tells the story of Amudha, a precocious nine-year old whose parents reveal to her that she was adopted, thus beginning an odyssey that takes them all from India to war-torn Sri Lanka. Gorgeous visuals mix with horrifying scenes of violence expressly to make a point, though it is a simplistic one. Amudha is played by P.S. Keerthana, and she is one of the few child actors I've seen who can be precocious and yet not annoying. Her charm and beauty held the film together.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Asia Argento is a sexy beautiful woman who likes to run around naked which isn't a bad thing in it's self, but, when her character talks about all the guys she serviced, and to see her with Michael Madsen and an Asian guy in the present tense of the movie, it made me feel like I needed a condom over my eyes to watch this movie, like a disease was going to rub off on me or something.<br /><br />The movie felt like it was going for a love triangle/drama/action/??? plot, it just seemed to go everywhere and nowhere at the same time. The acting was great, the plot, not so great. The director needs to at least pick a genre and practice, practice, practice, before trying to do something as complicated as this again, because they are not very good at it obviously.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I thought Sliver offered the most boring and trivial \"trick\" ending in movie history, but of course I was wrong. I had no disillusions that this movie was going to be good. Unfortunately, it was worse than I expected. The worst part is that the obvious ending is so ridiculous and horrible that you can\\'t believe it until it actually happens.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "valid_dataset = valid_pos_reviews.concatenate(valid_neg_reviews).shuffle(buffer_size = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch = valid_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_pos_reviews.concatenate(test_neg_reviews).shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple model to test the preprocessing layer defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, PReLU\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 50\n",
    "embedding_len = 100\n",
    "weight_reg = 0.03\n",
    "kernel_regularizer = regularizers.l2(weight_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = [], dtype = tf.string))\n",
    "model.add(Preprocessing_layer(vocab = vocab, num_oov_buckets = num_oov_buckets, \n",
    "                              embedding_len = embedding_len, \n",
    "                              kernel_regularizer = kernel_regularizer) )\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 256, activation = None, kernel_regularizer = kernel_regularizer))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 256, activation = None, kernel_regularizer = kernel_regularizer))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 64, activation = None, kernel_regularizer = kernel_regularizer))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 64, activation = None, kernel_regularizer = kernel_regularizer))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 16, activation = None, kernel_regularizer = kernel_regularizer))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 4, activation = None, kernel_regularizer = kernel_regularizer))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "preprocessing_layer (Preproc (None, 100)               7494300   \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 16)                16        \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 4)                 4         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 7,611,469\n",
      "Trainable params: 7,609,949\n",
      "Non-trainable params: 1,520\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/log_dir\n",
      "run file: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/log_dir/2020_03_11_00_06_40\n"
     ]
    }
   ],
   "source": [
    "# tensorboard logs\n",
    "import time\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'log_dir')\n",
    "print('log directory: \\n{}'.format(log_dir))\n",
    "run_id = time.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "run_file = os.path.join(log_dir, run_id)\n",
    "print('run file: \\n{}'.format(run_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for TensorBoard\n",
    "# check the following tutorial\n",
    "# https://www.tensorflow.org/tensorboard/get_started\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "monitor = 'val_loss'\n",
    "min_delta = 0.01\n",
    "patience = 10\n",
    "verbose = 1\n",
    "restore_best_weights = True\n",
    "stopper = EarlyStopping(monitor = monitor, min_delta = min_delta, \n",
    "                        patience = patience, verbose = verbose,\n",
    "                        restore_best_weights = restore_best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prarit/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/prarit/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 177s 354ms/step - loss: 10.5949 - accuracy: 0.8022 - val_loss: 1.1006 - val_accuracy: 0.7388\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 171s 343ms/step - loss: 0.6867 - accuracy: 0.8471 - val_loss: 0.7439 - val_accuracy: 0.8561\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 175s 349ms/step - loss: 0.5929 - accuracy: 0.8648 - val_loss: 0.5922 - val_accuracy: 0.8494\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 174s 348ms/step - loss: 0.5350 - accuracy: 0.8704 - val_loss: 0.7082 - val_accuracy: 0.8473\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 172s 345ms/step - loss: 0.5058 - accuracy: 0.8817 - val_loss: 0.7415 - val_accuracy: 0.8558\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 173s 346ms/step - loss: 0.4853 - accuracy: 0.8857 - val_loss: 0.6691 - val_accuracy: 0.7519\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 172s 345ms/step - loss: 0.5162 - accuracy: 0.8900 - val_loss: 0.5706 - val_accuracy: 0.8734\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 171s 343ms/step - loss: 0.4514 - accuracy: 0.8950 - val_loss: 0.5665 - val_accuracy: 0.8686\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 171s 343ms/step - loss: 0.4551 - accuracy: 0.8958 - val_loss: 0.7721 - val_accuracy: 0.5033\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 0.4911 - accuracy: 0.8974 - val_loss: 0.5480 - val_accuracy: 0.8721\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 173s 346ms/step - loss: 0.6111 - accuracy: 0.8976 - val_loss: 0.7862 - val_accuracy: 0.8714\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 172s 344ms/step - loss: 0.5376 - accuracy: 0.8976 - val_loss: 0.5707 - val_accuracy: 0.8689\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 174s 349ms/step - loss: 0.4559 - accuracy: 0.9010 - val_loss: 0.5004 - val_accuracy: 0.8801\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 174s 348ms/step - loss: 0.4353 - accuracy: 0.9001 - val_loss: 0.6284 - val_accuracy: 0.8251\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 172s 345ms/step - loss: 0.4521 - accuracy: 0.8994 - val_loss: 0.6115 - val_accuracy: 0.8700\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 172s 344ms/step - loss: 0.4385 - accuracy: 0.9027 - val_loss: 0.5639 - val_accuracy: 0.8658\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.4498 - accuracy: 0.9030 - val_loss: 0.5764 - val_accuracy: 0.8698\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 176s 352ms/step - loss: 0.4780 - accuracy: 0.9025 - val_loss: 0.6904 - val_accuracy: 0.8667\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.4279 - accuracy: 0.9056 - val_loss: 0.5605 - val_accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 172s 344ms/step - loss: 0.4351 - accuracy: 0.9072 - val_loss: 0.5188 - val_accuracy: 0.8722\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.4627 - accuracy: 0.9031 - val_loss: 0.7291 - val_accuracy: 0.8303\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 173s 346ms/step - loss: 0.4749 - accuracy: 0.9027 - val_loss: 0.6565 - val_accuracy: 0.8507\n",
      "Epoch 23/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.9048Restoring model weights from the end of the best epoch.\n",
      "500/500 [==============================] - 175s 349ms/step - loss: 0.4429 - accuracy: 0.9048 - val_loss: 0.5346 - val_accuracy: 0.8757\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "#### fit the model\n",
    "epochs = 50\n",
    "history = model.fit(train_batch, epochs = epochs, \n",
    "                    validation_data = valid_batch, \n",
    "                    callbacks = [stopper, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    250/Unknown - 57s 228ms/step - loss: 0.5116 - accuracy: 0.8710"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5116342711448669, 0.87096]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model on the test batch\n",
    "model.evaluate(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample review: \n",
      "b'outside sweden you are not expected have seen this movie happy you the cast includes several actors that are important part of modern the swedish movie history and still seems like peter dalle only had a an idea lasting for about 20 minutes robert gustafson is totally misused in this movie trying to copy a younger g\\xc3\\xb6sta ekman ekman by the way is the only actor fulfilling the expectations credit that can be given is for the photo splendid idea using black and white music is ok but over all it s a waste of god actors and the time of the audience'\n",
      " \n",
      "model prediction: [0.21980561]\n",
      " \n",
      "sample label: 0\n"
     ]
    }
   ],
   "source": [
    "# prediction for a single test sample\n",
    "import numpy as np\n",
    "\n",
    "for sample in test_batch.take(1):\n",
    "    preds = model.predict(sample[0])\n",
    "    \n",
    "print('sample review: \\n{}'.format(sample[0][0]))\n",
    "print(' ')\n",
    "print('model prediction: {}'.format(preds[0]))\n",
    "print(' ')\n",
    "print('sample label: {}'.format(sample[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize = (15,5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4679b657805b4841\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4679b657805b4841\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 9009;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=log_dir --port=9009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
