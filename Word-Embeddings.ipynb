{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create and train an embedding layer for the words appearing in a text data. We will then train a simple DNN based model to do sentiment analysis on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exercise 13.10 in [this](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "\n",
    "  - a. Download the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
    "  \n",
    "    \n",
    "  - b. Split the test set into a validation set (15,000) and a test set (10,000).\n",
    "  \n",
    "  \n",
    "  - c. Use tf.data to create an efficient dataset for each set.\n",
    "  \n",
    "  \n",
    "  - d. Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method.\n",
    "  \n",
    "  \n",
    "  - e. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n",
    "  \n",
    "  \n",
    "  - f. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.\n",
    "\n",
    "\n",
    "  - g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.1.0\n",
      "keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/prarit/MachineLearningProjects/Word-Embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('cwd: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good tutorial on using wget: https://www.tecmint.com/download-and-extract-tar-files-with-one-command/\n",
    "# turn off verbose output of wget using the flag -nv : https://shapeshed.com/unix-wget/#how-to-turn-off-verbose-output \n",
    "!wget -c -nv http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -o - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncompress the downloaded files\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tensorflow also provides this dataset: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md',\n",
       " '.gitignore',\n",
       " 'log_dir',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'aclImdb',\n",
       " 'Word-Embeddings.ipynb',\n",
       " 'aclImdb_v1.tar.gz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the files in the current working directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that aclImdb_v1.tar.gz was extracted to a folder called aclImdb. Let's see the contents of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of aclImdb are: \n",
      "['imdb.vocab', 'train', 'README', 'imdbEr.txt', 'test']\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd() , 'aclImdb')\n",
    "contents = os.listdir(path)\n",
    "print('The contents of aclImdb are: \\n{}'.format(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a README file in aclImdb, let us read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read README\n",
    "filepath = os.path.join(path, 'README')\n",
    "with open(filepath, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From README, we find that the train/test folder contains a 'pos' folder for positive reviews and a 'neg' for negative reviews along with .txt files containing urls of positive and negative reviews respectively. There are also some other files for bag-of-words features etc. \n",
    "\n",
    "Each train/test folder contains a total of 25000 reviews of which 12500 are positive reviews and 12500 are negative reviews.\n",
    "\n",
    "Let's verify the above about the 'train' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents of the train folder: \n",
      "['unsup', 'urls_pos.txt', 'pos', 'labeledBow.feat', 'unsupBow.feat', 'neg', 'urls_neg.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(path,'train')\n",
    "print('contents of the train folder: \\n{}'.format(os.listdir(train_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a dataset, we need the path to all the reviews. We can create the corresponding list of paths using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with positive reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the positive reviews in the training set\n",
    "train_pos_path = os.path.join(train_path, 'pos', '*.txt')\n",
    "train_pos_reviews = glob.glob(train_pos_path)\n",
    "print('No. of train-set files with positive reviews: {}'.format(len(train_pos_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the negative reviews in the training set\n",
    "train_neg_path = os.path.join(train_path, 'neg', '*.txt')\n",
    "train_neg_reviews = glob.glob(train_neg_path)\n",
    "print('No. of train-set files with negative reviews: {}'.format(len(train_neg_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us give a brief look at a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I watch them all.<br /><br />It's not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it's completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don't watch. 'How she move.' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It's just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I'm a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn't all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.\n"
     ]
    }
   ],
   "source": [
    "file = train_pos_reviews[0]\n",
    "with open(file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will like to load an preprocess all the data using tensorflow's data API, therefore let us quickly see how to read the same file as above but this time by using tensorflow's [TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'I watch them all.<br /><br />It\\'s not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it\\'s completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don\\'t watch. \\'How she move.\\' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It\\'s just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I\\'m a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn\\'t all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "pos_fl0 = tf.data.TextLineDataset(file)\n",
    "for item in pos_fl0:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As expected, we see that the pos_fl0 contains a single item and it's value output matches the output of the previous code cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having, learnt how to use tf.data.TextLineData() method, we can now starting preprocessing the data. In order to do this, we notice that the review contains punctuation marks and html line break tags etc. We will have to write a preprocessing function to get rid of these. Additionally, we will also change all alphabets to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing line-brk tags\n",
    "\n",
    "This can be very simply done by using the .replace() method of python strings. We can therefore use it to replace all occurrances of the line-break tag with a space. In tensorflow, the equivalent method is [tf.strings.regex_replace()](https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.strings.regex_replace\n",
    "Note that 'regex' in regex_replace() stands for [\"regular expression\"](https://docs.python.org/3/howto/regex.html). For e.g. the following will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('hello','e','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the following will throw an error: tf.strings.regex_replace('h(llo', '(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because \"(\" is a metacharacter. To match and replace meta-characters, we must prepend a backslash before them. This can be done as follows: '\\\\\\\\' + char. \n",
    "\n",
    "The previous code cell can now be made to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The error thrown by this code cell is intentional\n",
    "tf.strings.regex_replace('h(llo', '\\\\'+'(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h(llo','\\(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imp: \n",
    "\n",
    "Note that backslash itself is also a  meta-character. To search and replace backslash, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h\\llo', '\\\\'+'\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use '\\\\\\\\' and NOT '\\\\' to search and replace a backslash\n",
    "tf.strings.regex_replace('h\\llo', '\\\\\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try three different ways of removing punctuations from a tensorflow string and compare their timings:\n",
    "\n",
    "1) punc_filter_and_to_lower1: use tf.strings.unicode_decode() to convert all the characters in the string into an array of their ascii codes. We then loop through this array, skipping over the places where the entry matches the ascii code of a punctuation. Finally we call tf.strings.unicode_encode() on this array to convert the ascii codes back to characters, thereby obtaining a string with all the punctuations stripped. \n",
    "\n",
    "\n",
    "2) punc_filter_and_to_lower2: use tf.regex_replace() to search and replace each punctuation by empty space. Pay special attention to prepend backslash in order to be able to use meta-characters in regex_replace.\n",
    "\n",
    "\n",
    "3) punc_filter_and_to_lower3: simply extract the python string using its .numpy() method. Then simple iterate through the characters of the string, skipping over the punctutations. Join the resulting list of character using .join() method. \n",
    "\n",
    "4) punc_filter_and_to_lower4: Use [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) by setting it's \"alphanum_only\" arg to True. This way it will only parse alpha-numeric characters in the text and to split the text at occurrances of non-alphanumeric characters. Since whitespace is a non-alpha-numeric character, the output will largely consist of a list of words in the text.  The caeat with this approach is that words with apostrophe in them such as \" don't \" will be split into two words: \"don\" and \"t\". Also, [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) does NOT consider underscores as non-alpha-numeric, so underscores do not get removed form the text. On the other hand since, the tokenizer already generates a list of words, we use this list to generate a vocobulary of words in the dataset at this step it-self, making the preprocessing faster.\n",
    "\n",
    "There timing on the first review in the training set was as follows:\n",
    "\n",
    "1) punc_filter_and_to_lower1: Wall time: ~ 3 s\n",
    "\n",
    "2) punc_filter_and_to_lower2: Wall time: ~ 8 ms\n",
    "\n",
    "3) punc_filter_and_to_lower3: Wall time: ~ 6 ms\n",
    "\n",
    "4) punc_filter_and_to_lower4: Wall time: ~ 6 ms (when not udating a vocabulary of words) \n",
    "\n",
    "5) 4) punc_filter_and_to_lower4: Wall time: ~ 7 ms (when also udating a vocabulary of words) \n",
    "\n",
    "Clearly, the last function is the fastest with the first one being extremely slow (takes several secs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations before utf encoding: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "punctuations after utf encoding: \n",
      "[ 33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  58  59  60\n",
      "  61  62  63  64  91  92  93  94  95  96 123 124 125 126]\n"
     ]
    }
   ],
   "source": [
    "# list of punctuations\n",
    "punc_ls = string.punctuation\n",
    "print('punctuations before utf encoding: {}'.format(punc_ls))\n",
    "punc_ls2 = tf.strings.unicode_decode(punc_ls, input_encoding = 'UTF-8')\n",
    "print('punctuations after utf encoding: \\n{}'.format(punc_ls2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.unicode_encode(), tf.strings.unicode_decode()\n",
    "def punc_filter_and_to_lower1(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = tf.strings.unicode_decode(st2, input_encoding = 'utf-8')\n",
    "    st2 = tf.strings.unicode_encode([char for char in st2 if char not in punc_ls2], output_encoding = 'UTF-8')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 s, sys: 63.7 ms, total: 3.47 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower1(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.regex_replace()\n",
    "def punc_filter_and_to_lower2(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ')\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    for punc in punc_ls:\n",
    "        # V.imp: to replace meta-characters we prepend a backslash to them \n",
    "        # infact we can also prepend a backslach before all the characters\n",
    "        # to prepend a backslash before a character we do: '\\\\' + char\n",
    "        st2 = tf.strings.regex_replace(st2, '\\\\'+ punc, ' ')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.89 ms, sys: 160 Âµs, total: 9.05 ms\n",
      "Wall time: 8.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower2(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all  it s not better than the amazing ones   strictly ballroom    shall we dance    japanese version   but it s completely respectable and pleasingly different in parts  i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting  for example  the  name should scream don t watch   how she move   since when can movie titles ignore grammar    there is nothing inherently incorrect about caribbean english grammar  it s just not canadian standard english grammar  comments about the dialogue seem off to me  i put on the subtitles because i m a canadian standard english speaker  so i just automatically assumed that i would have trouble understanding all of it  it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american  i loved that this movie was set in toronto and  in fact  wish it was even more clearly set there  i loved that the heroine was so atypically cast  i enjoyed the stepping routines  i liked the driven mum character  i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies  in summary  if you tend to like dance movies  then this is a decent one  if you have superiority issues about the grammar of the english standard you grew up speaking  your narrow mind may have difficulty enjoying this movie '>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using .join() method in python string class\n",
    "def punc_filter_and_to_lower3(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = st2.numpy().decode('utf-8')\n",
    "    st2 = ''.join([char for char in st2 if char not in punc_ls])\n",
    "    \n",
    "    st2 = st2.lower()\n",
    "    \n",
    "    st2 = tf.convert_to_tensor(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 ms, sys: 3.16 ms, total: 6.48 ms\n",
      "Wall time: 5.81 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower3(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tfds.features.text.tokenizer()\n",
    "# we will also simultaneously generate a vocabulary in this step\n",
    "import tensorflow_datasets as tfds\n",
    "def punc_filter_and_to_lower4(st, vocab = None):\n",
    "    ''' vocab: vocabulary to update with the words in st'''\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.lower(tf.strings.regex_replace(st, line_brk_tag, ' '))\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    tokenizer = tfds.features.text.Tokenizer()\n",
    "    words = tokenizer.tokenize(st2.numpy()) # note that the inpout has to be a python string NOT as tensor \n",
    "                                            # The output is a list NOT a tensor\n",
    "                                            # https://stackoverflow.com/questions/56665868/tensor-numpy-not-working-in-tensorflow-data-dataset-throws-the-error-attribu\n",
    "    st2 = tf.strings.join(words, ' ')\n",
    "    \n",
    "    if type(vocab) == set:\n",
    "        vocab.update(words)\n",
    "        return st2, vocab\n",
    "    \n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.22 ms, sys: 0 ns, total: 7.22 ms\n",
      "Wall time: 6.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower4(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all it s not better than the amazing ones _strictly ballroom_ _shall we dance _ japanese version but it s completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream don t watch how she move since when can movie titles ignore grammar there is nothing inherently incorrect about caribbean english grammar it s just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because i m a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.79 ms, sys: 0 ns, total: 7.79 ms\n",
      "Wall time: 7.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test the timing of punc_filter_and_to_lower4 when simultaneously updating a dictionary\n",
    "vocab2 = set([])\n",
    "for item in pos_fl0:\n",
    "    lst, vocab2 = punc_filter_and_to_lower4(item, vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary based on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be easily done using python's [set()](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset) container.: basically, split each text in the training instance into words and update the set with this list. This will add any new words in the text to the set. In the end, the set will contain all the unique words in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have also implemented the above idea in our function punc_filter_and_to_lower4() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a vocabulary using the set() container\n",
    "vocab1 = set([]) # python set for containing unique words in the training dataset\n",
    "def vocab_builder1(strng):\n",
    "    # split the string into its words\n",
    "    words = tf.strings.split(strng)\n",
    "    # update vocab\n",
    "    vocab1.update(words.numpy())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 233 ms, sys: 70.8 ms, total: 304 ms\n",
      "Wall time: 302 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower3(item)\n",
    "    vocab_builder1(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocab_builder1() function as defined above takes about 300 ms. On the otherhand, in punc_filter_and_to_lower4() function,  we were able to get almost the same result (upto the caveats mentioned in the previous section) by using tfds.features.text.Tokenizer(), in about 8 ms. Clearly, there is a difference of few order of magnitude between the time taken by the two routines with the difference between their output being quite tolerable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================="
     ]
    }
   ],
   "source": [
    "vocab = set([])\n",
    "train_filepaths = tf.data.Dataset.list_files([train_pos_path, train_neg_path])\n",
    "train_dataset = tf.data.TextLineDataset(train_filepaths)\n",
    "\n",
    "ctr = 0\n",
    "for item in train_dataset:\n",
    "    _ , vocab = punc_filter_and_to_lower4(item, vocab)\n",
    "    \n",
    "    ctr+=1\n",
    "    if ctr%255 ==0:\n",
    "        print(\"=\", end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mastering', '1861', 'ditz', 'scwarz', 'intercepting']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick 5 random words from vocab to inspect it\n",
    "import random\n",
    "\n",
    "random.sample(vocab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words in vocabulary: 74893\n"
     ]
    }
   ],
   "source": [
    "print('no. of words in vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a lookup table based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "lookup_initializer = tf.lookup.KeyValueTensorInitializer( list(vocab), indices) \n",
    "num_oov_buckets = 50 \n",
    "lookup_table = tf.lookup.StaticVocabularyTable( initializer = lookup_initializer, \n",
    "                                               num_oov_buckets = num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map a text to integers using the above lookup table\n",
    "def text_to_integers(X):\n",
    "    X = tf.strings.split(X)\n",
    "    int_ls = lookup_table.lookup(X)\n",
    "    return int_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[46532 45075 74022 46331 53524 64720 19317 62409 73522 16143 44242 41577\n",
      " 50874 68983 31313 15087  8306 21664 13123 33468 13306 53524 64720 65527\n",
      " 71776 66995 14238 36541  7772 58213 46532 73888 53036 67119 62983 66995\n",
      " 46532 42335 52981 16827 16143 22743 57728 58416  7772 52981 16827 18980\n",
      " 16927 45262 54275 13214 65814 16143 29277 29030 19133 54210 64365 45075\n",
      " 53001 16072 24642 56047 45093  4951 14832  2865 50949 31021  2131 26411\n",
      " 41707 13786 56795 57728 55127 67119 31021 53524 64720 57294 19317 33442\n",
      " 56790 67119 31021 32260 57728 16143 13415 52656 71145  4299 33307 46532\n",
      " 11617 70284 16143 58245 42062 46532 10547 26462 33442 56790 67119 33148\n",
      " 15287 46532 57294 35139 60301 65609 46532 71361 38725  8242 35308 46331\n",
      " 16827 53524 53524 18322 64365 46331 65609  5258 66995 53524 34038 26462\n",
      " 57757 36541 32626  3957 16143 43034 55205 20619 46532 38725 42984 38430\n",
      " 15287 55800 46532 63046 65609 51505 14832 36927 33312  7772 53837 66995\n",
      "  7772 11067 48930 53524 36927 62366 64192 34158 33312  2131 46532 63046\n",
      " 65609 16143 53468 36927 15287 34361 54631 46532  7872 16143 33595 16319\n",
      " 46532 69998 16143 64470 72880   809 46532 57756 65609 44705 16827 16143\n",
      " 49944  7772 16143 14832 38430 62388 64192 52637 73522 26411 56743 16827\n",
      "  8306 20619  7772 11121 52234  3539 20643  4299 41798  8306 20619 29241\n",
      " 51505 26411 26462 14584 10851 52234  3539 38725 74519 49944 57728 16143\n",
      " 31021 16827 16143 67119 56790  3539 47002 50600  9545 19874 12595 18732\n",
      " 73740 38725 17993 35984 51505 14832], shape=(246,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(74943, 100) dtype=float32, numpy=\n",
       "array([[0.84739673, 0.5330745 , 0.8537576 , ..., 0.8085828 , 0.19710445,\n",
       "        0.61763525],\n",
       "       [0.570151  , 0.21181452, 0.9607618 , ..., 0.32157898, 0.81303966,\n",
       "        0.78700495],\n",
       "       [0.29486346, 0.33803856, 0.10440004, ..., 0.62405384, 0.34108508,\n",
       "        0.79526067],\n",
       "       ...,\n",
       "       [0.58884156, 0.9760572 , 0.5741606 , ..., 0.8563837 , 0.6412374 ,\n",
       "        0.4897573 ],\n",
       "       [0.7385638 , 0.00988424, 0.33112252, ..., 0.14830363, 0.21763122,\n",
       "        0.18210769],\n",
       "       [0.10461211, 0.7373018 , 0.51076746, ..., 0.3489182 , 0.43539405,\n",
       "        0.11802483]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "initial_embeddings = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim] ) \n",
    "embedding_matrix = tf.Variable(initial_embeddings)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an embedding-vector from a list of word indices\n",
    "def indices_to_embeddings(idxs):\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix, idxs)\n",
    "    sqrt_num_words = tf.math.sqrt(tf.cast(tf.size(idxs), tf.float32)) # size is of type int32 but \n",
    "                                                                      # tf.math.sqrt has to have one of \n",
    "                                                                      # bfloat16, half, float32, \n",
    "                                                                      # float64, complex64, \n",
    "                                                                      # complex128.\n",
    "    mean_embedding = tf.math.reduce_mean(embeddings, axis = 0)\n",
    "    sentence_embedding = tf.multiply(sqrt_num_words, mean_embedding)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    item = indices_to_embeddings(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([8.693973 , 8.854761 , 7.82933  , 8.957902 , 7.2097936, 7.3304343,\n",
       "       8.380673 , 7.3754983, 7.6155996, 8.670477 , 6.968219 , 7.8654613,\n",
       "       7.5966697, 7.502631 , 7.268158 , 8.001531 , 7.877151 , 7.0359006,\n",
       "       7.3503532, 8.404755 , 9.161611 , 8.507119 , 7.9875207, 7.132951 ,\n",
       "       8.258061 , 7.2016726, 8.300606 , 7.6019235, 7.66854  , 7.5146866,\n",
       "       7.952212 , 6.7936077, 7.91821  , 7.4322057, 7.7793183, 7.798514 ,\n",
       "       7.3226557, 7.3620768, 7.9715447, 8.720195 , 7.8295207, 7.957666 ,\n",
       "       7.8305845, 7.7825203, 7.7725887, 7.9687223, 7.9044704, 7.7980723,\n",
       "       8.581489 , 8.405485 , 8.449623 , 7.231178 , 7.63637  , 7.9109426,\n",
       "       7.703827 , 8.627375 , 7.282163 , 8.5807   , 7.225756 , 8.835483 ,\n",
       "       9.227416 , 7.8802958, 6.968203 , 7.4323564, 7.3603973, 6.811419 ,\n",
       "       8.219535 , 7.566163 , 7.795026 , 7.252293 , 8.265123 , 8.780831 ,\n",
       "       7.650981 , 7.909001 , 7.5872536, 7.48621  , 7.622642 , 6.997355 ,\n",
       "       7.7961073, 8.10192  , 7.7592072, 7.8313136, 8.170635 , 8.0945215,\n",
       "       7.941437 , 7.065291 , 7.3634467, 7.0623016, 8.442152 , 7.862655 ,\n",
       "       8.548494 , 7.483838 , 8.233758 , 7.9572515, 7.261374 , 7.793054 ,\n",
       "       7.186136 , 6.97054  , 8.436645 , 7.3323708], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessing layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a preprocessing layer that accepts a sentence as an input, removes punctuations and line-brk tags form it, then converts it's words into indices and finally output an embedding vector corresponding to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_layer(keras.layers.Layer): \n",
    "    def __init__(self, vocab,  num_oov_buckets, embedding_len, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab = list(vocab)\n",
    "        self.vocab_len  = tf.size(self.vocab, out_type = tf.int64) \n",
    "        self.num_oov_buckets = num_oov_buckets\n",
    "        self.embedding_len = embedding_len\n",
    "        self.indices = tf.range(self.vocab_len, dtype = tf.int64)\n",
    "        self.lookup_initializer = tf.lookup.KeyValueTensorInitializer(self.vocab, \n",
    "                                                                      self.indices)\n",
    "        self.lookup_table = tf.lookup.StaticVocabularyTable(initializer = self.lookup_initializer, \n",
    "                                                            num_oov_buckets = self.num_oov_buckets)\n",
    "         \n",
    "    # build the embedding matrix \n",
    "    def build(self, batch_input_shape):\n",
    "        self.embedding_mat = self.add_weight(name = 'embedding matrix', \n",
    "                                             shape = [self.vocab_len + self.num_oov_buckets, self.embedding_len],\n",
    "                                             initializer = tf.random_uniform_initializer)\n",
    "        \n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    \n",
    "    # function to convert a sentence into a list of indices for its words\n",
    "    def text_to_integers(self, X):\n",
    "        X_words = tf.strings.split(X)\n",
    "        # X_words will be a ragged tensor\n",
    "        # on the otherhand, self.lookup_table.lookup() expects either a Sparse tensor or a dense tensor\n",
    "        # for example see the documentation: https://www.tensorflow.org/api_docs/python/tf/lookup/StaticVocabularyTable\n",
    "        # in order to use it ragged tensors, we have to use tf.ragged.map_flay_values()\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/ragged/map_flat_values\n",
    "        X_int = tf.ragged.map_flat_values(self.lookup_table.lookup, X_words)\n",
    "        return X_int\n",
    "    \n",
    "    # function to convert a list of indices into an embedding vector\n",
    "    def integers_to_embeddings(self, X):\n",
    "        sqrt_num_words = tf.linalg.diag(\n",
    "            tf.math.sqrt(tf.cast(X.nested_row_lengths(), tf.float32))[0])\n",
    "        X_ems = tf.nn.embedding_lookup(self.embedding_mat, X)\n",
    "        X_vec = tf.math.reduce_mean(X_ems, axis = 1) # note that X_ems is a ragged tensor whose \n",
    "                                                     # 0-th dim. corresponds to the batch dim. \n",
    "                                                     # therefore axis = 1 and NOT 0 when taking mean\n",
    "        \n",
    "        return sqrt_num_words@X_vec\n",
    "        \n",
    "    # define the call method\n",
    "    def call(self, X):\n",
    "        \n",
    "        # recall X contains strings with all the punctuations etc removed\n",
    "        # Imp: The shape of X is (batch_size, ...) i.e. the 0-th axis of X corresponds to batch dim. \n",
    "        # We first convert all its words into their indices\n",
    "        X_int = self.text_to_integers(X)\n",
    "        # now convert this a mean embedding vector\n",
    "        X_vec = self.integers_to_embeddings(X_int)\n",
    "        \n",
    "        return X_vec\n",
    "    \n",
    "    \n",
    "    # compute output shape\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.embedding_len])\n",
    "    \n",
    "    \n",
    "    # add the layers hyperparameters to the configuration file\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'embedding_len': self.embedding_len, \n",
    "                'num_oov_buckets': self.num_oov_buckets}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the positive and negative reviews in each dataset have been stored in seperate folders and as such they have no explicit labels. We will therefore have to create our own labels as was done in [this](https://www.tensorflow.org/tutorials/load_data/text) official tensorflow tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return apply a label\n",
    "def labeler(review, label):\n",
    "    return review, tf.cast(label, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_paths = tf.data.Dataset.list_files(train_pos_path)\n",
    "train_pos_reviews = tf.data.TextLineDataset(train_pos_paths).map(lambda x : labeler(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Cinderella is a beautiful film, with beautiful songs of course. In fact, it's one of the best films of the 1950's.<br /><br />I think all the characters are portrayed amazingly. You can see the cruelness of Cinderella's stepsisters and her stepmother, the sweetness of Cinderella. The mice are funny and sweet too.<br /><br />I think they changed the tale a bit, but I think it's for the best. It's such a nice film, and I don't think anyone could resist it deep down.<br /><br />I give it a 8/10. I don't think it's the best Disney film. But it sure is a true classic.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"White man + progress + industrialization = BAD. First nations + nature + animals = GOOD. Simple formula. Actually, in past days the same kind of propaganda was used to defend the status quo; now it is used to attack it. However, that being said, I think the movie does succeed in overcoming hackneyed politicization because it plays to the themes of freedom and original nature in a way that appeals to everyone. You may not be onside with the movie's rubbishy revisionism of how the West was won, er lost. But anyone can feel a sense of longing for the days when horses could run free on the Western plains. (The movie also conveniently sidesteps the fact that there were no horses in America before the evil white man brought them there). Anyway, I liked it. The quality of the animation - especially the opening shot - is incredible.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_paths = tf.data.Dataset.list_files(train_neg_path)\n",
    "train_neg_reviews = tf.data.TextLineDataset(train_neg_paths).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"It's boggles the mind how this movie was nominated for seven Oscars and won one. Not because it's abysmal or because given the collective credentials of the creative team behind it really ought to deserve them but because in every category it was nominated Prizzi's Honor disappoints. Some would argue that old Hollywood pioneer John Huston had lost it by this point in his career but I don't buy it. Only the previous year he signed the superb UNDER THE VOLCANO, a dark character study set in Mexico, that ranks among the finest he ever did. Prizzi's Honor on the other hand, a film loaded with star power, good intentions and a decent script, proves to be a major letdown.<br /><br />The overall tone and plot of a gangster falling in love with a female hit-man prefigures the quirky crimedies that caught Hollywood by storm in the early 90's but the script is too convoluted for its own sake, the motivations are off and on the whole the story seems unsure of what exactly it's trying to be: a romantic comedy, a crime drama, a gangster saga etc. Jack Nicholson (doing a Brooklyn accent that works perfectly for De Niro but sounds unconvincing coming from Jack) and Kathleen Turner in the leading roles seem to be in paycheck mode, just going through the motions almost sleepwalking their way through some parts. Anjelica Huston on the other hand fares better but her performance is sabotaged by her character's motivations: she starts out the victim of her bigot father's disdain, she proves to be supportive to her ex-husband, then becomes a vindictive bitch that wants his head on a plate.<br /><br />The colours of the movie have a washed-up quality like it was made in the early 70's and Huston's direction is as uninteresting as everything else. There's promise behind the story and perhaps in the hands of a director hungry to be recognized it could've been morphed to something better but what's left looks like a film nobody was really interested in making.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Joyce Reynolds seems a might grown-up for the role of Janie, a boy-crazy sixteen-year old in small town America who ditches her steady guy for a visiting soldier AND winds up on the cover of Life magazine (smooching at a blanket party) all in the same week! Non-stop barrage of wisecracks, put-downs, bull talk, and unfunny bits of business such as Janie\\'s little sister bribing family members, Hattie McDaniel (as the maid) constantly scuttling after sassy kid sis, Janie\\'s mother involved with the Red Cross, and Janie\\'s father trying to write an editorial on the problems with today\\'s teenagers (as the parents, stuffy, sexless Edward Arnold and pert, chatty Ann Harding make an unlikely couple, even for 1944; he looks incapable of helping to conceive a child much less raising two of them). Nominated for an Academy Award (!) for Owen Marks\\' editing, Warner Bros. followed this in 1946 with \"Janie Gets Married\". Reynolds must have outgrown her co-horts by then--she was replaced by Joan Leslie. *1/2 from ****'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_buffer_size = 20000\n",
    "train_dataset = train_pos_reviews.concatenate(train_neg_reviews).shuffle(buffer_size = shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in train_dataset:\n",
    "    count+=1\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to apply punc_filter_and_to_lower4() to the reviews in our training dataset. For this we will use the [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method of tf.data.Datasets. At this point it is important to note that (as mentioned in the documentation for [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)):\n",
    "\n",
    "    Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have two options:\n",
    "\n",
    "     1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.\n",
    "\n",
    "     2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is of concern to us because the [tokenizer](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) we used in punc_filter_and_to_lower4() only accepts string input and NOT tensors. This string was extracted from the tensor using its .numpy() method which is only operable in eager mode. Thus when we pass punc_filter_and_to_lower4() to map(), it throws an error, complaining:\n",
    "\n",
    "     'Tensor' object has no attribute 'numpy'\n",
    "\n",
    "\n",
    "Thus we have to wrap punc_filter_and_to_lower4() with tf.py_function before passing it to map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to wrap punc_filter_and_to_lower4 with tf.py_function\n",
    "# we will also use tf.ensure_shape(), or else tensorflow is \n",
    "# unable to statically determine it, leading to error being thrown during training\n",
    "def string_transform(X):\n",
    "    x = tf.py_function(punc_filter_and_to_lower4, [X], Tout = tf.string)\n",
    "    x = tf.ensure_shape(x, ())\n",
    "    \n",
    "    return x\n",
    "\n",
    "batch_size = 50\n",
    "prefetch = 2\n",
    "train_batch = train_dataset.map(lambda X, y: \n",
    "                                    (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have alternatively, preprocessed the data using punc_filter_and_to_lower2() which is based on tensorflow functions and then used it. The price we would have to pay is to rebuild the vocabulary accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train-batch: (50,)\n",
      "tf.Tensor(b'basically this movie is one of those rare movies you either hate and think borders on suicide as the next best thing to do rather than having to sit through it for two hours or as in my case you see it as a kult hit one of those movies wherein the humour the plot the acting is actually very hidden but for those of us willing to go looking for it trusting the director well the reward is u laugh your a of the fact that u have to find the things mentioned above actually makes the movie even more funny because u get the impression the director isn t even aware of how funny his movie is which doesn t seem likely and therein lies the intelligence at the helm of this magnificient project called spaced invaders', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ite = next(iter(train_batch))\n",
    "print('shape of train-batch: {}'.format(ite[0].shape))\n",
    "print(ite[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       "array([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ite[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed in the excercise, we will create a validation dataset by splitting the test set into half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test\n",
      "path to positive test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/pos/*.txt\n",
      "path to negative test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/neg/*.txt\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(path, 'test')\n",
    "print(test_path)\n",
    "test_pos_path = os.path.join(test_path, 'pos', '*.txt')\n",
    "print('path to positive test reviews: \\n{}'.format(test_pos_path))\n",
    "test_neg_path = os.path.join(test_path, 'neg', '*.txt')\n",
    "print('path to negative test reviews: \\n{}'.format(test_neg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_files = glob.glob(test_pos_path)\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = 12500\n",
    "valid_pos_files = random.sample(test_pos_files, int(valid_size/2))\n",
    "len(valid_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can remove the valid_pos_files from the set of test_pos_files through the\n",
    "# simple trick explained in the following post:\n",
    "# https://stackoverflow.com/questions/6486450/python-compute-list-difference/6486467\n",
    "test_pos_files = list(set(test_pos_files) - set(valid_pos_files))\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial test_neg_files len:12500\n",
      "valid_neg_files len: 6250\n",
      "test_neg_files len: 6250\n"
     ]
    }
   ],
   "source": [
    "test_neg_files = glob.glob(test_neg_path)\n",
    "print('initial test_neg_files len:{}'.format(len(test_neg_files)))\n",
    "valid_neg_files = random.sample(test_neg_files, int(valid_size/2))\n",
    "print('valid_neg_files len: {}'.format(len(valid_neg_files)))\n",
    "test_neg_files = list(set(test_neg_files) - set(valid_neg_files))\n",
    "print('test_neg_files len: {}'.format(len(test_neg_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pos_reviews = tf.data.TextLineDataset(valid_pos_files).map(lambda x: labeler(x, 1))\n",
    "valid_neg_reviews = tf.data.TextLineDataset(valid_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'This is the best version of Gypsy that has been filmed.Bette Midler is simply superb as Mama Rose.She has the voice,the gestures,the look,and most of all,a supreme acting ability to carry the role off and to make her character come alive.Her singing is,simply put:MAGNIFICENT! She especially shines in two numbers-\"Everything\\'s Coming Up Roses\" and \"Rose\\'s Turn\". The other actors are also fine,particularly Christine Ebersoll as Tessie.Also good is Peter Riegert;his portrayal of Herbie is acted with great style and believability.The direction of this movie is very,very good.There isn\\'t a false note or gaffe in the entire production.This film is a vast improvement over the Roz Russell version filmed in 1962.Since viewing it again,I can state that the three greatest Mama Roses are:Ethel Merman,Bette Midler,and Angela Lansbury. See this movie.You\\'ll be glad that you did.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Of the Korean movies I've seen, only three had really stuck with me. The first is the excellent horror A Tale of Two Sisters. The second and third - and now fourth too - have all been Park Chan Wook's movies, namely Oldboy, Sympathy for Lady Vengeance), and now Thirst. <br /><br />Park kinda reminds me of Quentin Tarantino with his irreverence towards convention. All his movies are shocking, but not in a gratuitous sense. It's more like he shows us what we don't expect to see - typically situations that go radically against society's morals, like incest or a libidinous, blood-sucking, yet devout priest. He's also quite artistically-inclined with regards to cinematography, and his movies are among the more gorgeous that I've seen.<br /><br />Thirst is all that - being about said priest and the repressed, conscience-less woman he falls for - and more. It's horror, drama, and even comedy, as Park disarms his audience with many inappropriate yet humorous situations. As such, this might be his best work for me yet, since his other two movies that I've seen were lacking the humor element that would've made them more palatable for repeat viewings.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I've only seen most of the series since I leave the TV on as background noise in my dorm.<br /><br />I've been a fan of Mencia but this show really doesn't do much for me. Occasionally he'll say or do something to pull a chuckle out, but he has this aura of smugness that completely ruins it.<br /><br />I've always thought he was funny because of his raging angry-man routine that's not terribly prevalent in this TV series. Instead, he's just smug. I guess that just reflects how funny his comedy is: stale and uninteresting when he isn't in the proper mode of delivery. I've seen him get into it sometimes on his show, but for the most part, he just sits there smiling and looking smug, and it doesn't suit him well.<br /><br />Just my opinion though.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'As a recent convert to Curb Your Enthusiasm, which prompted my viewing of all season\\'s episodes, I expected more, much more from Jeff\\'s efforts.<br /><br />When I view a film offering a slice of an average \\'Joe\\'s\\' life I need reasons to be interested, to care, to feel and believe. And with Jeff Garlin at the helm I also expected a bevy of shining comedic moments. This film failed me time and again.<br /><br />Jeff plays a living with mom, plump sad sack who is a social disaster. He has not had a relationships, real or even casual, for many years. He appears to be mostly unemployed and, as noted, shacks with his mommy dearest. Can things get worse? Sure. In short order he gets sacked by everyone around him including Silverman, Second City (his comedy workshop) and his agent. All reinforce his \\'loser\\' status. Silverman\\'s \\'fatty\\' experiment was as cruel as it was absurd. His obsession with the role of \"Marty,\" as the means of his career\\'s salvation, also hits a big dead end.<br /><br />While the film\\'s final moments offer a glimpse of better things to come the cinematic \\'journey,\\' albeit with occasional golden glimmers, was sadly lacking.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_reviews = tf.data.TextLineDataset(test_pos_files).map(lambda x: labeler(x, 1))\n",
    "test_neg_reviews = tf.data.TextLineDataset(test_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Mom has to be one of the all time uncomfortable movies to watch. It features an elderly lady you would love to have as your Nanny who becomes the nastiest mother f***ing monster you would ever want to meet on a dark night!<br /><br />This supper Nanny eats the inners of a young lady at the opening of the movie and it just gets sicker as it goes on. A cross between the howling and brain dead seem to come to mind when describing Mom!<br /><br />A must for horror fans who have the stomach for it (if you have watched re-animator or brain dead, this will float your boat)and are willing to switch the brain off for an hour or so...Let the gore pour!...8/10'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I haven\\'t seen every single movie that Burt Reynolds has ever made, but this one (which I\\'ve just finished watching, for the third time) may very well be his best! It suffers only from some slow stretches; Burt perhaps tried to make it more \"arty\" than it should have been. On the other hand, he managed to avoid many of the usual cliches in the presentation of the \"tough cop\" role he plays (notice, for example, the scene in which he attempts to kiss Rachel Ward for the first time, or the fear he expresses just before the final showdown with the indestructible Henry Silva). In fact, Silva and those two ninja assassins are three of the most memorable villains of cop thrillers of the 80s. The film also has some offbeat touches, a surprising amount of humor, a brutal and gripping fistfight and many well-directed shots. (***)'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'This was included on the disk \"Shorts: Volume 2\"--a rather dull collection of short films. Shorts are among my favorite style of films but somehow the people assembling this second collection had a hard time finding quality content--and it wasn\\'t nearly as good as the first volume or other shorts collections. <br /><br />This short film feels like it\\'s woefully incomplete. There is a story, but so much in unanswered that the viewer, like me, feels a bit left out and unfulfilled.<br /><br />The film begins with a woman, her boyfriend and her Westie (that\\'s a dog, by the way) going to a lonely beach. The lady speaks with an accent that, at times, is a bit difficult to follow. Given that I am hard of hearing, I sure would have loved if it had been closed captioned. Anyway, the boyfriend goes for a swim while she naps. When she awakens, her dog is gone. She panics and makes the guy follow her all about looking for the dog. They spend most of the time arguing and being disagreeable. Then, out of the blue, they stop to have sex. Later, they find the dog--end of story.<br /><br />As far as the characters go, both seemed rather dysfunctional and unlikable. She was a fussy and demanding lady and he seemed to have contempt for her. When you wondered why they were together, their little sex break showed what bond kept them together.<br /><br />Some might like the characterizations--I kept finding the people irritating and unreal--more like caricatures than people you might meet or know. Also, the payoff for all this just isn\\'t worth the wait (unless you want to see the guy naked).'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I have not figured out what the chosen title has to do with the movie. This is another gathering of monsters just like the HOUSE OF FRANKENSTEIN. Not exactly a masterful plot, but Universal needed to capitalize again.<br /><br />Dr. Edelman (Onslow Stevens) is either very ambitious or over the top in the ego department. He is working on the cure to keep Larry Talbot from turning into the Wolf Man. Somehow Count Dracula happens to drop by to get a fix on his vampirism. And rounding out the good doctor's experiments is the restoring of the Frankenstein monster's energy. Along the way, the kind hearted doctor's blood is tainted with that of Dracula.<br /><br />John Carradine plays Dracula again. This time he is more convincing. Lon Chaney Jr. as usual is the soulful Wolf Man. Glenn Strange is the Frankenstein monster, who has very little to do this outing. Also with mentionable roles are Lionel Atwill and Martha O'Driscoll.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "valid_dataset = valid_pos_reviews.concatenate(valid_neg_reviews).shuffle(buffer_size = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch = valid_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_pos_reviews.concatenate(test_neg_reviews).shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple model to test the preprocessing layer defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 50\n",
    "embedding_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = [], dtype = tf.string))\n",
    "model.add(Preprocessing_layer(vocab = vocab, \n",
    "                              num_oov_buckets = num_oov_buckets, \n",
    "                              embedding_len = embedding_len) )\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 256, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 256, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 64, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 64, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 16, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 4, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "preprocessing_layer_4 (Prepr (None, 100)               7494300   \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 16)                16        \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 4)                 4         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 7,611,469\n",
      "Trainable params: 7,609,949\n",
      "Non-trainable params: 1,520\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/log_dir\n",
      "run file: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/log_dir/2020_m_10_21_09_42\n"
     ]
    }
   ],
   "source": [
    "# tensorboard logs\n",
    "import time\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'log_dir')\n",
    "print('log directory: \\n{}'.format(log_dir))\n",
    "run_id = time.strftime('%Y_m_%d_%H_%M_%S')\n",
    "run_file = os.path.join(log_dir, run_id)\n",
    "print('run file: \\n{}'.format(run_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for TensorBoard\n",
    "# check the following tutorial\n",
    "# https://www.tensorflow.org/tensorboard/get_started\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "monitor = 'val_loss'\n",
    "min_delta = 0.01\n",
    "patience = 10\n",
    "verbose = 1\n",
    "restore_best_weights = True\n",
    "stopper = EarlyStopping(monitor = monitor, min_delta = min_delta, \n",
    "                        patience = patience, verbose = verbose,\n",
    "                        restore_best_weights = restore_best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 176s 351ms/step - loss: 0.4062 - accuracy: 0.8097 - val_loss: 0.3334 - val_accuracy: 0.8591\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 174s 348ms/step - loss: 0.1743 - accuracy: 0.9364 - val_loss: 0.3381 - val_accuracy: 0.8630\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 172s 343ms/step - loss: 0.0947 - accuracy: 0.9669 - val_loss: 0.4152 - val_accuracy: 0.8512\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 172s 344ms/step - loss: 0.0670 - accuracy: 0.9771 - val_loss: 0.4736 - val_accuracy: 0.8492\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 174s 348ms/step - loss: 0.0517 - accuracy: 0.9812 - val_loss: 0.5610 - val_accuracy: 0.8445\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 174s 349ms/step - loss: 0.0451 - accuracy: 0.9850 - val_loss: 0.5036 - val_accuracy: 0.8526\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 174s 348ms/step - loss: 0.0302 - accuracy: 0.9899 - val_loss: 0.5816 - val_accuracy: 0.8502\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 174s 349ms/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.5768 - val_accuracy: 0.8618\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 173s 345ms/step - loss: 0.0191 - accuracy: 0.9944 - val_loss: 0.7096 - val_accuracy: 0.8440\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 175s 350ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.5661 - val_accuracy: 0.8549\n",
      "Epoch 11/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9944Restoring model weights from the end of the best epoch.\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.5850 - val_accuracy: 0.8589\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "#### fit the model\n",
    "epochs = 50\n",
    "history = model.fit(train_batch, epochs = epochs, \n",
    "                    validation_data = valid_batch, \n",
    "                    callbacks = [stopper, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
