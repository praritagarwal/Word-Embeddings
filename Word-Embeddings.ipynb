{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create and train an embedding layer for the words appearing in a text data. We will then train a simple DNN based model to do sentiment analysis on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exercise 13.10 in [this](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "\n",
    "  - a. Download the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
    "  \n",
    "    \n",
    "  - b. Split the test set into a validation set (15,000) and a test set (10,000).\n",
    "  \n",
    "  \n",
    "  - c. Use tf.data to create an efficient dataset for each set.\n",
    "  \n",
    "  \n",
    "  - d. Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method.\n",
    "  \n",
    "  \n",
    "  - e. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n",
    "  \n",
    "  \n",
    "  - f. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.\n",
    "\n",
    "\n",
    "  - g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.1.0\n",
      "keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/prarit/MachineLearningProjects/Word-Embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('cwd: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good tutorial on using wget: https://www.tecmint.com/download-and-extract-tar-files-with-one-command/\n",
    "# turn off verbose output of wget using the flag -nv : https://shapeshed.com/unix-wget/#how-to-turn-off-verbose-output \n",
    "!wget -c -nv http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -o - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncompress the downloaded files\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tensorflow also provides this dataset: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'aclImdb',\n",
       " 'Word-Embeddings.ipynb',\n",
       " 'aclImdb_v1.tar.gz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the files in the current working directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that aclImdb_v1.tar.gz was extracted to a folder called aclImdb. Let's see the contents of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of aclImdb are: \n",
      "['imdb.vocab', 'train', 'README', 'imdbEr.txt', 'test']\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd() , 'aclImdb')\n",
    "contents = os.listdir(path)\n",
    "print('The contents of aclImdb are: \\n{}'.format(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a README file in aclImdb, let us read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read README\n",
    "filepath = os.path.join(path, 'README')\n",
    "with open(filepath, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From README, we find that the train/test folder contains a 'pos' folder for positive reviews and a 'neg' for negative reviews along with .txt files containing urls of positive and negative reviews respectively. There are also some other files for bag-of-words features etc. \n",
    "\n",
    "Each train/test folder contains a total of 25000 reviews of which 12500 are positive reviews and 12500 are negative reviews.\n",
    "\n",
    "Let's verify the above about the 'train' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents of the train folder: \n",
      "['unsup', 'urls_pos.txt', 'pos', 'labeledBow.feat', 'unsupBow.feat', 'neg', 'urls_neg.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(path,'train')\n",
    "print('contents of the train folder: \\n{}'.format(os.listdir(train_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a dataset, we need the path to all the reviews. We can create the corresponding list of paths using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with positive reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the positive reviews in the training set\n",
    "train_pos_path = os.path.join(train_path, 'pos', '*.txt')\n",
    "train_pos_reviews = glob.glob(train_pos_path)\n",
    "print('No. of train-set files with positive reviews: {}'.format(len(train_pos_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the negative reviews in the training set\n",
    "train_neg_path = os.path.join(train_path, 'neg', '*.txt')\n",
    "train_neg_reviews = glob.glob(train_neg_path)\n",
    "print('No. of train-set files with negative reviews: {}'.format(len(train_neg_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us give a brief look at a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I watch them all.<br /><br />It's not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it's completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don't watch. 'How she move.' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It's just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I'm a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn't all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.\n"
     ]
    }
   ],
   "source": [
    "file = train_pos_reviews[0]\n",
    "with open(file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will like to load an preprocess all the data using tensorflow's data API, therefore let us quickly see how to read the same file as above but this time by using tensorflow's [TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'I watch them all.<br /><br />It\\'s not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it\\'s completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don\\'t watch. \\'How she move.\\' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It\\'s just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I\\'m a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn\\'t all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "pos_fl0 = tf.data.TextLineDataset(file)\n",
    "for item in pos_fl0:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As expected, we see that the pos_fl0 contains a single item and it's value output matches the output of the previous code cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having, learnt how to use tf.data.TextLineData() method, we can now starting preprocessing the data. In order to do this, we notice that the review contains punctuation marks and html line break tags etc. We will have to write a preprocessing function to get rid of these. Additionally, we will also change all alphabets to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing line-brk tags\n",
    "\n",
    "This can be very simply done by using the .replace() method of python strings. We can therefore use it to replace all occurrances of the line-break tag with a space. In tensorflow, the equivalent method is [tf.strings.regex_replace()](https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.strings.regex_replace\n",
    "Note that 'regex' in regex_replace() stands for [\"regular expression\"](https://docs.python.org/3/howto/regex.html). For e.g. the following will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('hello','e','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the following will throw an error: tf.strings.regex_replace('h(llo', '(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because \"(\" is a metacharacter. To match and replace meta-characters, we must prepend a backslash before them. This can be done as follows: '\\\\\\\\' + char. \n",
    "\n",
    "The previous code cell can now be made to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The error thrown by this code cell is intentional\n",
    "tf.strings.regex_replace('h(llo', '\\\\'+'(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h(llo','\\(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imp: \n",
    "\n",
    "Note that backslash itself is also a  meta-character. To search and replace backslash, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h\\llo', '\\\\'+'\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use '\\\\\\\\' and NOT '\\\\' to search and replace a backslash\n",
    "tf.strings.regex_replace('h\\llo', '\\\\\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try three different ways of removing punctuations from a tensorflow string and compare their timings:\n",
    "\n",
    "1) punc_filter_and_to_lower1: use tf.strings.unicode_decode() to convert all the characters in the string into an array of their ascii codes. We then loop through this array, skipping over the places where the entry matches the ascii code of a punctuation. Finally we call tf.strings.unicode_encode() on this array to convert the ascii codes back to characters, thereby obtaining a string with all the punctuations stripped. \n",
    "\n",
    "\n",
    "2) punc_filter_and_to_lower2: use tf.regex_replace() to search and replace each punctuation by empty space. Pay special attention to prepend backslash in order to be able to use meta-characters in regex_replace.\n",
    "\n",
    "\n",
    "3) punc_filter_and_to_lower3: simply extract the python string using its .numpy() method. Then simple iterate through the characters of the string, skipping over the punctutations. Join the resulting list of character using .join() method. \n",
    "\n",
    "4) punc_filter_and_to_lower4: Use [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) by setting it's \"alphanum_only\" arg to True. This way it will only parse alpha-numeric characters in the text and to split the text at occurrances of non-alphanumeric characters. Since whitespace is a non-alpha-numeric character, the output will largely consist of a list of words in the text.  The caeat with this approach is that words with apostrophe in them such as \" don't \" will be split into two words: \"don\" and \"t\". Also, [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) does NOT consider underscores as non-alpha-numeric, so underscores do not get removed form the text. On the other hand since, the tokenizer already generates a list of words, we use this list to generate a vocobulary of words in the dataset at this step it-self, making the preprocessing faster.\n",
    "\n",
    "There timing on the first review in the training set was as follows:\n",
    "\n",
    "1) punc_filter_and_to_lower1: Wall time: ~ 3 s\n",
    "\n",
    "2) punc_filter_and_to_lower2: Wall time: ~ 8 ms\n",
    "\n",
    "3) punc_filter_and_to_lower3: Wall time: ~ 6 ms\n",
    "\n",
    "4) punc_filter_and_to_lower4: Wall time: ~ 6 ms (when not udating a vocabulary of words) \n",
    "\n",
    "5) 4) punc_filter_and_to_lower4: Wall time: ~ 7 ms (when also udating a vocabulary of words) \n",
    "\n",
    "Clearly, the last function is the fastest with the first one being extremely slow (takes several secs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations before utf encoding: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "punctuations after utf encoding: \n",
      "[ 33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  58  59  60\n",
      "  61  62  63  64  91  92  93  94  95  96 123 124 125 126]\n"
     ]
    }
   ],
   "source": [
    "# list of punctuations\n",
    "punc_ls = string.punctuation\n",
    "print('punctuations before utf encoding: {}'.format(punc_ls))\n",
    "punc_ls2 = tf.strings.unicode_decode(punc_ls, input_encoding = 'UTF-8')\n",
    "print('punctuations after utf encoding: \\n{}'.format(punc_ls2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.unicode_encode(), tf.strings.unicode_decode()\n",
    "def punc_filter_and_to_lower1(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = tf.strings.unicode_decode(st2, input_encoding = 'utf-8')\n",
    "    st2 = tf.strings.unicode_encode([char for char in st2 if char not in punc_ls2], output_encoding = 'UTF-8')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.43 s, sys: 46.6 ms, total: 3.47 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower1(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.regex_replace()\n",
    "def punc_filter_and_to_lower2(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ')\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    for punc in punc_ls:\n",
    "        # V.imp: to replace meta-characters we prepend a backslash to them \n",
    "        # infact we can also prepend a backslach before all the characters\n",
    "        # to prepend a backslash before a character we do: '\\\\' + char\n",
    "        st2 = tf.strings.regex_replace(st2, '\\\\'+ punc, ' ')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.49 ms, sys: 93 µs, total: 8.58 ms\n",
      "Wall time: 7.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower2(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all  it s not better than the amazing ones   strictly ballroom    shall we dance    japanese version   but it s completely respectable and pleasingly different in parts  i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting  for example  the  name should scream don t watch   how she move   since when can movie titles ignore grammar    there is nothing inherently incorrect about caribbean english grammar  it s just not canadian standard english grammar  comments about the dialogue seem off to me  i put on the subtitles because i m a canadian standard english speaker  so i just automatically assumed that i would have trouble understanding all of it  it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american  i loved that this movie was set in toronto and  in fact  wish it was even more clearly set there  i loved that the heroine was so atypically cast  i enjoyed the stepping routines  i liked the driven mum character  i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies  in summary  if you tend to like dance movies  then this is a decent one  if you have superiority issues about the grammar of the english standard you grew up speaking  your narrow mind may have difficulty enjoying this movie '>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using .join() method in python string class\n",
    "def punc_filter_and_to_lower3(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = st2.numpy().decode('utf-8')\n",
    "    st2 = ''.join([char for char in st2 if char not in punc_ls])\n",
    "    \n",
    "    st2 = st2.lower()\n",
    "    \n",
    "    st2 = tf.convert_to_tensor(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.65 ms, sys: 121 µs, total: 4.77 ms\n",
      "Wall time: 4.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower3(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tfds.features.text.tokenizer()\n",
    "# we will also simultaneously generate a vocabulary in this step\n",
    "import tensorflow_datasets as tfds\n",
    "def punc_filter_and_to_lower4(st, vocab = None):\n",
    "    ''' vocab: vocabulary to update with the words in st'''\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.lower(tf.strings.regex_replace(st, line_brk_tag, ' '))\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    tokenizer = tfds.features.text.Tokenizer()\n",
    "    words = tokenizer.tokenize(st2.numpy()) # note that the inpout has to be a python string NOT as tensor \n",
    "                                            # The output is a list NOT a tensor\n",
    "                                            # https://stackoverflow.com/questions/56665868/tensor-numpy-not-working-in-tensorflow-data-dataset-throws-the-error-attribu\n",
    "    st2 = tf.strings.join(words, ' ')\n",
    "    \n",
    "    if type(vocab) == set:\n",
    "        vocab.update(words)\n",
    "        return st2, vocab\n",
    "    \n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.57 ms, sys: 0 ns, total: 7.57 ms\n",
      "Wall time: 6.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower4(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all it s not better than the amazing ones _strictly ballroom_ _shall we dance _ japanese version but it s completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream don t watch how she move since when can movie titles ignore grammar there is nothing inherently incorrect about caribbean english grammar it s just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because i m a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.49 ms, sys: 209 µs, total: 7.69 ms\n",
      "Wall time: 7.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test the timing of punc_filter_and_to_lower4 when simultaneously updating a dictionary\n",
    "vocab2 = set([])\n",
    "for item in pos_fl0:\n",
    "    lst, vocab2 = punc_filter_and_to_lower4(item, vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary based on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be easily done using python's [set()](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset) container.: basically, split each text in the training instance into words and update the set with this list. This will add any new words in the text to the set. In the end, the set will contain all the unique words in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have also implemented the above idea in our function punc_filter_and_to_lower4() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a vocabulary using the set() container\n",
    "vocab1 = set([]) # python set for containing unique words in the training dataset\n",
    "def vocab_builder1(strng):\n",
    "    # split the string into its words\n",
    "    words = tf.strings.split(strng)\n",
    "    # update vocab\n",
    "    vocab1.update(words.numpy())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 269 ms, sys: 55.1 ms, total: 325 ms\n",
      "Wall time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower3(item)\n",
    "    vocab_builder1(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocab_builder1() function as defined above takes about 300 ms. On the otherhand, in punc_filter_and_to_lower4() function,  we were able to get almost the same result (upto the caveats mentioned in the previous section) by using tfds.features.text.Tokenizer(), in about 8 ms. Clearly, there is a difference of few order of magnitude between the time taken by the two routines with the difference between their output being quite tolerable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================="
     ]
    }
   ],
   "source": [
    "vocab = set([])\n",
    "train_filepaths = tf.data.Dataset.list_files([train_pos_path, train_neg_path])\n",
    "train_dataset = tf.data.TextLineDataset(train_filepaths)\n",
    "\n",
    "ctr = 0\n",
    "for item in train_dataset:\n",
    "    _ , vocab = punc_filter_and_to_lower4(item, vocab)\n",
    "    \n",
    "    ctr+=1\n",
    "    if ctr%255 ==0:\n",
    "        print(\"=\", end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okej', 'strewn', 'critiscism', 'gloating', 'zit']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick 5 random words from vocab to inspect it\n",
    "import random\n",
    "\n",
    "random.sample(vocab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words in vocabulary: 74893\n"
     ]
    }
   ],
   "source": [
    "print('no. of words in vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a lookup table based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "lookup_initializer = tf.lookup.KeyValueTensorInitializer( list(vocab), indices) \n",
    "num_oov_buckets = 50 \n",
    "lookup_table = tf.lookup.StaticVocabularyTable( initializer = lookup_initializer, \n",
    "                                               num_oov_buckets = num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map a text to integers using the above lookup table\n",
    "def text_to_integers(X):\n",
    "    X = tf.strings.split(X)\n",
    "    int_ls = lookup_table.lookup(X)\n",
    "    return int_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[14407 58873  4975 69751 23242 53985 26925 74297 74002  7234 33777 14061\n",
      " 24305 41997 28979 10854 22754 17037  9513 71062 74253 23242 53985   666\n",
      " 21575 31566  9440 68180 51457 72708 14407 43549  4556 57045 33588 31566\n",
      " 14407  2899  3443 53730  7234 57531 35046 27833 51457  3443 53730 73798\n",
      " 61774 65314 53323 67509 11690  7234 48045 16591 49577 38757 54738 58873\n",
      " 67806 62527 43295 26847 73294   904 66560 69518  9479 69467 42413 67404\n",
      " 12509 12102 13956 35046 18379 57045 69467 23242 53985 40530 26925 61199\n",
      " 46624 57045 69467 49538 35046  7234 46449 32013 71807  3809 55572 14407\n",
      " 18524 51385  7234 49297  5559 14407 51597  9114 61199 46624 57045  4189\n",
      " 29750 14407 40530 39666 61590 28164 14407 49750 55549 39889  4438 69751\n",
      " 53730 23242 23242 65499 54738 69751 28164 28451 31566 23242 73744  9114\n",
      " 17745 68180 33195 29302  7234 74301 46796 71236 14407 55549   135 14822\n",
      " 29750 53187 14407 59298 28164 68165 66560 27927 52059 51457 22376 31566\n",
      " 51457 34050 19358 23242 27927 14912 46269 57474 52059 42413 14407 59298\n",
      " 28164  7234 60119 27927 29750 60131 31683 14407 59868  7234 35056 53906\n",
      " 14407 41973  7234  5765 12942 32230 14407 26034 28164  5446 53730  7234\n",
      " 31050 51457  7234 66560 14822 25013 46269 38872 74002 67404 63328 53730\n",
      " 22754 71236 51457 18587 21168 64318 26291  3809  7034 22754 71236  7262\n",
      " 68165 67404  9114  7390 40322 21168 64318 55549 47322 31050 35046  7234\n",
      " 69467 53730  7234 57045 46624 64318 44791 54650 20311 20202 28002  9524\n",
      " 18519 55549 36052 59634 68165 66560], shape=(246,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(74943, 100) dtype=float32, numpy=\n",
       "array([[0.27821112, 0.9385762 , 0.7214955 , ..., 0.6498685 , 0.8854619 ,\n",
       "        0.30109215],\n",
       "       [0.6042515 , 0.19952965, 0.9578979 , ..., 0.3977008 , 0.5859591 ,\n",
       "        0.9243921 ],\n",
       "       [0.591275  , 0.46339405, 0.59744716, ..., 0.5441897 , 0.73397696,\n",
       "        0.5774808 ],\n",
       "       ...,\n",
       "       [0.0693773 , 0.39532983, 0.7935848 , ..., 0.16767669, 0.54799306,\n",
       "        0.5737673 ],\n",
       "       [0.8727343 , 0.50011694, 0.7078893 , ..., 0.99221504, 0.9465153 ,\n",
       "        0.63276553],\n",
       "       [0.12636709, 0.40288007, 0.18929994, ..., 0.7660638 , 0.52223146,\n",
       "        0.05493701]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "initial_embeddings = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim] ) \n",
    "embedding_matrix = tf.Variable(initial_embeddings)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an embedding-vector from a list of word indices\n",
    "def indices_to_embeddings(idxs):\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix, idxs)\n",
    "    sqrt_num_words = tf.math.sqrt(tf.cast(tf.size(idxs), tf.float32)) # size is of type int32 but \n",
    "                                                                      # tf.math.sqrt has to have one of \n",
    "                                                                      # bfloat16, half, float32, \n",
    "                                                                      # float64, complex64, \n",
    "                                                                      # complex128.\n",
    "    mean_embedding = tf.math.reduce_mean(embeddings, axis = 0)\n",
    "    sentence_embedding = tf.multiply(sqrt_num_words, mean_embedding)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    item = indices_to_embeddings(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([7.8804903, 7.9428205, 8.321264 , 8.704269 , 8.907119 , 7.297222 ,\n",
       "       7.8233023, 8.103945 , 8.492896 , 7.055711 , 7.928911 , 7.2358847,\n",
       "       7.948384 , 7.3605404, 8.088991 , 8.656651 , 6.813101 , 7.492557 ,\n",
       "       7.6973166, 7.2751703, 7.442708 , 8.090645 , 6.679673 , 6.9293613,\n",
       "       8.752965 , 7.1476398, 7.7636847, 7.5437517, 8.52123  , 7.0815763,\n",
       "       8.814794 , 8.5974655, 8.198473 , 6.6910563, 6.8116493, 9.128407 ,\n",
       "       7.8052697, 8.204603 , 7.209249 , 7.2705297, 7.812039 , 7.356413 ,\n",
       "       8.261715 , 7.5293274, 8.118498 , 8.420315 , 8.838265 , 7.810327 ,\n",
       "       8.174669 , 7.1887155, 7.638866 , 7.5661545, 7.3809075, 7.9660535,\n",
       "       7.982779 , 7.776999 , 7.387608 , 7.168912 , 8.28974  , 7.2365117,\n",
       "       8.203039 , 7.347867 , 7.7058563, 8.465302 , 7.2081594, 7.844795 ,\n",
       "       8.32972  , 7.518017 , 8.138895 , 7.9976125, 7.2368646, 8.569483 ,\n",
       "       7.309824 , 7.5620494, 7.581903 , 8.855443 , 8.150174 , 7.189464 ,\n",
       "       7.6103034, 7.541956 , 8.344752 , 7.765208 , 7.7530146, 7.4157586,\n",
       "       7.57376  , 7.747303 , 7.8935194, 7.9575696, 7.8608274, 7.46935  ,\n",
       "       7.6278973, 7.5825515, 7.1590753, 8.851838 , 8.219838 , 7.8372836,\n",
       "       8.726864 , 7.7761064, 7.46833  , 7.5755363], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessing layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a preprocessing layer that accepts a sentence as an input, removes punctuations and line-brk tags form it, then converts it's words into indices and finally output an embedding vector corresponding to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_layer(keras.layers.Layer): \n",
    "    def __init__(self, vocab,  num_oov_buckets, embedding_len, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_len  = tf.size(vocab) \n",
    "        self.num_oov_buckets = num_oov_buckets\n",
    "        self.embedding_len = embedding_len\n",
    "        self.indices = tf.range(self.vocab_len, dtype = tf.int64)\n",
    "        self.lookup_intializer = tf.lookup.KeyValueTensorInitializer(list(self.vocab),\n",
    "                                                                     self.indices)\n",
    "        self.lookup_table = tf.lookup.StaticVocabularyTable(initializer = self.lookup_initializer, \n",
    "                                                            num_oov_buckets = self.num_oov_buckets)\n",
    "         \n",
    "    # build the embedding matrix \n",
    "    def build(self, batch_input_shape):\n",
    "        self.embedding_mat = self.add_weight(name = 'embedding matrix', \n",
    "                                             shape = [self.vocab_len + self.num_oov_buckets, self.embedding_len],\n",
    "                                             initializer = tf.random_uniform_initializer)\n",
    "        \n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    \n",
    "    # function to convert a sentence into a list of indices for its words\n",
    "    def text_to_integers(self, X):\n",
    "        X = tf.strings.split(X)\n",
    "        return lookup_table.lookup(X)\n",
    "    \n",
    "    # function to convert a list of indices into an embedding vector\n",
    "    def integers_to_embeddings(self, X):\n",
    "        sqrt_num_words = tf.math.sqrt(tf.cast(tf.size(X), tf.float32))\n",
    "        X = tf.nn.embedding_lookup(self.embedding_mat, X)\n",
    "        X = tf.math.reduce_mean(X, axis = 0) \n",
    "        \n",
    "        return tf.multiply(sqrt_num_words, X)\n",
    "        \n",
    "    # define the call method\n",
    "    def call(self, X):\n",
    "        \n",
    "        # recall X is a string with all the punctuations etc removed\n",
    "        # We first convert all its words into their indices\n",
    "        X = self.text_to_integers(X)\n",
    "        # now convert this a mean embedding vector\n",
    "        X = self.integers_to_embeddings(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the positive and negative reviews in each dataset have been stored in seperate folders and as such they have no explicit labels. We will therefore have to create our own labels as was done in [this](https://www.tensorflow.org/tutorials/load_data/text) official tensorflow tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return apply a label\n",
    "def labeler(review, label):\n",
    "    return review, tf.cast(label, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_paths = tf.data.Dataset.list_files(train_pos_path)\n",
    "train_pos_reviews = tf.data.TextLineDataset(train_pos_paths).map(lambda x : labeler(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'You will marvel at the incredibly sophisticated computer animation, and the novelty probably won\\'t wear off on the first, second or third viewing, but you?ll be drawn in by the characters which are so simple yet intriguing, that you may find yourself actually caring for them in an unexpected way, which may or may not make you feel a little childish due to the medium.<br /><br />Disney continues to firmly hold the title of \"Greatest Animation in the World\", with \"A Bug?s Life\" standing as one of their greatest achievements. One of the innovative attachments being the delightful \"out-takes\" added to the end of the film. The DVD has two sets of these out-takes where as I?m told the VHS cassette has one alternating version per tape. The DVD also features \"Gerry?s Game\" which is a delightful little PIXAR short that was also shown prior to the film in theaters.<br /><br />This is by far the superior insect-film in comparison to Dreamworks? \"Antz\", which in all fairness is pretty good, but lacks something in the animation and in the story development and characters. If you look at the star voices of both films, \"Antz\" is largely cast with big name \"movie\" stars with a few familiar \"TV\" star voices, where \"A Bug?s Life\" is just the opposite, loaded with \"TV\" stars with Kevin Spacey as the only stand out exception. But the difference in quality is distinct and obvious.<br /><br />Dreamworks can?t be blamed or surprised though, when you go head to head with Disney, you have your work cut out for you. This is the kind of film that almost makes me wish I had children to share it with. Don?t think for a second that this is just a movie for kids, though.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Barbra Streisand's first television special was simply fantastic! From her skit as a child to her medley of songs in a high-fashion department store -- everything was top-notch! It was easy to understand how this special received awards.<br /><br />Not muddled down by guest appearances, the focus remained on Barbra thoughout the entire production.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_paths = tf.data.Dataset.list_files(train_neg_path)\n",
    "train_neg_reviews = tf.data.TextLineDataset(train_neg_paths).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Pictures that usually glorify a hero have meaning. As an example, Bonnie and Clyde glorified the dynamic bank robbers and you actually felt sympathy for them despite their evil deeds. Why? They were two people caught up in the depression when people were desperate to survive.<br /><br />This film has absolutely no substance. The Viggo Mortensen character soon emerges as a folk hero. Why? He speeds along an Idaho highway on the way to the hospital where his stricken wife has been taking. No one bothers to understand why he is trying to flee everyone. Even worse, when the realization becomes apparent that he is not a red-neck terrorist, no one in government wants to help him as they try to save their rear ends.<br /><br />Jason Priestley co-stars as a radio emcee who builds upon the story in support of our hero.<br /><br />The ending is absolutely unbelievable.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Exactly what you would expect from a B-Movie. Deritive, gratuitous nudity, boring in parts, ridiculous gore and cheesy special-effects. Of course it could have been better, better acted (defintly) better written, directed, etc. But then I guess it wouldn't have been a B movie. The actors pretty much sucked, in fact this pretty much seemed like an episode of buffy the vampire slayer or something except with a lot of blood, profanity and nudity.<br /><br />Tiffany Shepis must be singled out. She absolutely is the scream queen of the new millennium. Not that acting really matters in these movies, but she was better than any of the other actors. She's also smokin hot, in that plastic jump suit thing she wore for the whole movie - wow! Her posterior is absolutely stunning in that outfit, I mean it every single time she turns around you can help but check her out. And near then end of the film the viewer is rewarded with seeing her completely nekked.<br /><br />So if your a looser b-movie horror buff (like myself), check this out. If not, you should probably avoid at all costs.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in train_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_buffer_size = 20000\n",
    "train_dataset = train_pos_reviews.concatenate(train_neg_reviews).shuffle(buffer_size = shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in train_dataset:\n",
    "    count+=1\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to apply punc_filter_and_to_lower4() to the reviews in our training dataset. For this we will use the [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method of tf.data.Datasets. At this point it is important to note that (as mentioned in the documentation for [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)):\n",
    "\n",
    "    Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have two options:\n",
    "\n",
    "     1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.\n",
    "\n",
    "     2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is of concern to us because the [tokenizer](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) we used in punc_filter_and_to_lower4() only accepts string input and NOT tensors. This string was extracted from the tensor using its .numpy() method which is only operable in eager mode. Thus when we pass punc_filter_and_to_lower4() to map(), it throws an error, complaining:\n",
    "\n",
    "     'Tensor' object has no attribute 'numpy'\n",
    "\n",
    "\n",
    "Thus we have to wrap punc_filter_and_to_lower4() with tf.py_function before passing it to map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to wrap punc_filter_and_to_lower4 with tf.py_function\n",
    "# we will also use tf.ensure_shape(), or else tensorflow is \n",
    "# unable to statically determine it, leading to error being thrown during training\n",
    "def string_transform(X):\n",
    "    x = tf.py_function(punc_filter_and_to_lower4, [X], Tout = tf.string)\n",
    "    x = tf.ensure_shape(x, ())\n",
    "    \n",
    "    return x\n",
    "\n",
    "batch_size = 50\n",
    "prefetch = 2\n",
    "train_batch = train_dataset.map(lambda X, y: \n",
    "                                    (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have alternatively, preprocessed the data using punc_filter_and_to_lower2() which is based on tensorflow functions and then used it. The price we would have to pay is to rebuild the vocabulary accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train-batch: (50,)\n",
      "tf.Tensor(b'i hate to sound like an old person but frankly i haven t seen too many movies that i like that were made after 1960 generally movies just seem to get worse and worse although i quite enjoyed the scott baio vehicle the bread my sweet except for the de rigeur sex scene which added nothing of value to that movie this movie makes the mother a former las vegas chorus girl seem to be incapable of surviving on her own although she is clearly in her 50s though hinted at being in her 40s i didn t buy it i m 57 and like all the women i know in their 50s and 40s more than capable of surviving on my own as i have been doing since i graduated from high school at 13 got legally emancipated and set off on my own life s journey the daughter is not believable in her job role she gets a promotion she doesn t deserve a great opportunity and drops that ball too but when another female employee steps up to the plate and is ready to deliver the writers shoot her down as an opportunist when she was just doing what any career oriented person would do taking advantage of a wide open opportunity created by the lack of self discipline of her coworker a girl who apparently doesn t understand the concept of honoring her promises to her boss in this case the daughter grudgingly allows her mother to stay with her on a temporary basis but then treats her mother the woman who gave her life and raised her to adulthood like a pariah apparently the writers of tripe like this do not understand that it is not the common thing for parents to act like children and then be treated as children by their children that is just more of the societal baloney that hollywood keeps trying to force down our throats as though we their public were stupid for desiring to be entertained by their creative offerings this is a sad movie with a stupid ending if the young male restauranteur had been real and not a two dimensional tv character he d have stayed with the mother who was not that much older than him and quite attractive but in the end he falls for the daughter a shallow rather uninteresting girl who has that cuteness of youth but in an ordinary bland way the opportunist young woman who worked with this nothing girl was far more attractive physically there was no believable reason presented to the audience as to why the restauranteur preferred the daughter who was an uptight selfish self centered b tch who treated her mother with unbelievable disrespect to the mother a woman who was kindhearted sweet tempered humorous and had a joie de vivre the daughter could not even begin to comprehend of course the mother had her own flaws she had reacted to her husband s demise by drinking herself into a stupor for a year or two afterwards which supposedly created the rift between her and her smarmy daughter regardless of the way the characters were or were not developed this is a baloney movie and a waste of your valuable viewing time unless you actually like baloney where s the mustard', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ite = next(iter(train_batch))\n",
    "print('shape of train-batch: {}'.format(ite[0].shape))\n",
    "print(ite[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ite[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed in the excercise, we will create a validation dataset by splitting the test set into half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test\n",
      "path to positive test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/pos/*.txt\n",
      "path to negative test reviews: \n",
      "/home/prarit/MachineLearningProjects/Word-Embeddings/aclImdb/test/neg/*.txt\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(path, 'test')\n",
    "print(test_path)\n",
    "test_pos_path = os.path.join(test_path, 'pos', '*.txt')\n",
    "print('path to positive test reviews: \\n{}'.format(test_pos_path))\n",
    "test_neg_path = os.path.join(test_path, 'neg', '*.txt')\n",
    "print('path to negative test reviews: \\n{}'.format(test_neg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_files = glob.glob(test_pos_path)\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = 12500\n",
    "valid_pos_files = random.sample(test_pos_files, int(valid_size/2))\n",
    "len(valid_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can remove the valid_pos_files from the set of test_pos_files through the\n",
    "# simple trick explained in the following post:\n",
    "# https://stackoverflow.com/questions/6486450/python-compute-list-difference/6486467\n",
    "test_pos_files = list(set(test_pos_files) - set(valid_pos_files))\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial test_neg_files len:12500\n",
      "valid_neg_files len: 6250\n",
      "test_neg_files len: 6250\n"
     ]
    }
   ],
   "source": [
    "test_neg_files = glob.glob(test_neg_path)\n",
    "print('initial test_neg_files len:{}'.format(len(test_neg_files)))\n",
    "valid_neg_files = random.sample(test_neg_files, int(valid_size/2))\n",
    "print('valid_neg_files len: {}'.format(len(valid_neg_files)))\n",
    "test_neg_files = list(set(test_neg_files) - set(valid_neg_files))\n",
    "print('test_neg_files len: {}'.format(len(test_neg_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pos_reviews = tf.data.TextLineDataset(valid_pos_files).map(lambda x: labeler(x, 1))\n",
    "valid_neg_reviews = tf.data.TextLineDataset(valid_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"It was high time a movie about the situation in a largely ignored Asian country like Myanmar had to be made and Beyond Rangoon is Hollywood's answer. Initially I thought Hollywood would dramatize the events of the 8888 uprising and add in the traditionally Hollywood spice of Titanic-type love between the lead heroine and the Burmese male lead who happens to be an old man. Thankfully, nothing of that sort was in place - which may also explain why the film was not financially successful.<br /><br />Anyway, the film was honest-to-God and I was glad at the accuracy of events portrayed. Apart from the fact that filming was done outside of Myanmar in Malaysia & Thailand and that I missed the exotic Burmese locales, I could not find much fault in the film.<br /><br />You cannot blame the film for the desperation of the people and the resulting overwhelming actions. It is after all, real events of a civil war. The music by Hans Zimmer is definitely the USP of an otherwise adventurous tragedy for people who have no connection to it.<br /><br />I was only a year old in Rangoon (now Yangon) during this tumultuous time. When I heard a movie was made on my real-life experience which I was too young to absorb, I had to get the DVD and needless to say, I could hardly have any complaints about it as it is an eye-opening wonder for me.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Those prophetic words were spoken by William Holden (as a war reporter) to the beauteous Jennifer Jones (as a Eurasian doctor), explaining his failing marriage on the beach. They start an affair, despite huge odds of adultery and racial issues. In Hollywood of the 1950s, interracial romance was allowed but only with dire consequences at the end. Beautiful Hong Kong scenery (although some beach scenes look studio-bound), a famous title tune, poetic script, lovely background music (by Alfred Newman), great costumes, outstanding performances, especially Jones (directed here by Henry King, who also did \"The Song of Bernadette - 1943, an Oscar for Jones) still make this a world-class romance weeper.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I cannot believe I actually set up a 'season pass' on my TiVo for this, apparently they had a good preview or something .. I can't imagine it though. After seeing about 5 minutes I thought to myself.. why am I watching this.. It is definitely not reality, and some of the worst acting I have ever seen on television.. I am a total addict of reality TV and there is nothing real about this. THE ACTING.. (if you can call it that) is awful.. The only ones that are almost 5% decent are the girls that are meant to draw viewers to the show.. although they would need to be a lot prettier to save this train wreck.. if they would have more lines they would probably change my mind as they probably have no talent either. This is obviously a very low budget production with 'actors' who are apparently very cheap. There is no way they could get a job in anything else. Someone needs to direct these people to a job in food service.. Maybe they could do that. Oh and by the way, Parco P.I. no longer occupies any space on my TiVo or TiVo's season pass list. Definitely the worst show I've ever seen.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Perhaps it\\'s just me, but this movie seemed more like sequel or follow-up than the separate project. Why? When it was filmed (just few years after the war) most of the viewers probably knew why Rommel was so famous, why his death was so important to Allied, why he was Hitler\\'s favorite general, but now, 50 years later, it isn\\'t so obvious anymore.<br /><br />\"Desert Fox: The Story of Rommel\" is a decent war movie, but it\\'s just isn\\'t in any way explained how Rommel did get his nickname, what was he doing that Allied considered him as their best general, why their soldiers were so afraid of Afrika Korps? That\\'s what is missing in this movie - we see his fame, his character, his way to treat soldiers and enemies, but f.e. we also see that Hitler was complaining about his achievements in Africa, calling him coward, etc. So, we\\'re missing the big picture here - it is \"The Story of Rommel\", but unfortunately the \"Desert Fox\" part is missing.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in valid_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_reviews = tf.data.TextLineDataset(test_pos_files).map(lambda x: labeler(x, 1))\n",
    "test_neg_reviews = tf.data.TextLineDataset(test_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Harry Langdon\\'s \"Saturday Afternoon\" is often ranked among the greatest silent comedies, at least where short subjects are concerned, and therefore may come as a bit of a letdown for some. Unlike some of the other recognized classics such as Keaton\\'s \"Cops\" or Chaplin\\'s \"The Immigrant\" this film is in some respects a familiar, conventional situation comedy and doesn\\'t offer much in the way of belly laughs; one may even wonder whether Langdon belongs in such rarefied company. Nonetheless, in my opinion, it\\'s a perfectly charming comedy in its minor-key way, and Harry is fascinating to watch.<br /><br />For a modern viewer raised on TV sitcoms the plot of \"Saturday Afternoon\" may suggest The Honeymooners or its many spin-offs: two dim guys, one of whom is married and very much under his wife\\'s thumb, try to sneak out with a couple of good-time girls for a fun afternoon; but everything goes wrong, and they wind up having to fight the girls\\' tough guy boyfriends. Does this sound familiar? And perhaps a little dreary? Well, the premise was already shopworn when this film was made, but beyond that nothing about Langdon was typical. He was odd, starting with the fact that he looked like a middle-aged baby who was half asleep. Any Freudians who catch \"Saturday Afternoon\" will have a field day with the scenes between this timid, pudgy-faced baby-man and his stern, gently domineering mommy-wife. When Harry tries to hide money under the rug but she catches him in the act and forces him to hand it over, you\\'d swear you\\'re watching an interaction between a 6 year-old boy and his Mama . . . and maybe that\\'s why Harry Langdon gave some people the creeps, and still does.<br /><br />But he\\'s a compelling screen figure, and it\\'s not what he does so much as the way he does it. In that scene with the coins under the rug, for instance, Harry finds the coins by placing one foot before the other, carefully, like a tightrope walker, counting off his paces until he finds the right spot, and his technique is hypnotic. Langdon moved like no one else. Whether or not he makes you laugh, the guy is mesmerizing, seemingly in a world of his own. Where the plot of his films is concerned Harry is curiously passive, and almost never drives the story forward himself. In the finale of \"Saturday Afternoon,\" when the big fistfight is taking place, Harry\\'s co-star Vernon Dent is in the thick of the action, but Harry is in a daze for much of the time, and winds up sort of punch-drunk between two cars (sitting on the running board of one, but with his feet on the other) while they race through the streets. It\\'s a memorable image, and, as the critic Walter Kerr wrote, it encapsulates Langdon\\'s screen persona quite perfectly: he\\'s a passive figure who somehow finds himself in the middle of frantic action, blinking sleepily while the world rushes past. It\\'s also worth noting that Langdon and Dent, who worked together frequently, have a rapport in this movie that suggests a blueprint Laurel & Hardy would follow when they teamed up a year or so later. Langdon\\'s style was a likely influence on Stan Laurel, especially here.<br /><br />\"Saturday Afternnon\" and its star may not be for everyone, but the film is well worth a look, and you might find that Langdon makes an impression that\\'s hard to shake.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I\\'m usually disappointed by what the media dubs \"lesbian\" movies these days: murderous bisexuals; psychotic murderous lesbians; women who experiment with other women, but end up with men at the end; ridiculously good-looking women who only get w/ each other to turn men on, etc.<br /><br />Thankfully, FINGERSMITH is on a very high pedestal above this garbage. It is a credible love story acted MARVELOUSLY by every cast member, down to the least of the supporting actors. Aside from having a very engaging central conflict, the romance between the heroines is well developed and believable thanks to Cassidy and Hawkins.<br /><br />I have also seen TIPPING THE VELVET, but FINGERSMITH is far superior to the former, both in character/conflict development and the quality of the acting.<br /><br />FINGERSMITH is both satisfying and enjoyable to watch, offering lesbians everywhere a great follow-up act to BOUND.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This movie is, in all likelihood, the worst film ever made. It is certainly the worst that I've ever seen, and I have seen A LOT of bad movies.<br /><br />In this, nothing at all interesting happens throughout the movie. One could, literally, start the movie, take a short nap, and then wake up secure in the knowledge that nothing interesting has happened while you are sleeping. And I have seen this movie three times, staying awake throughout. I feel I should be congratulated.<br /><br />The movie goes as one might expect, according to a formula, with no variation. Hunters capture baby bigfoot, get killed by parent, and the nearby town goes into a bigfoot killing frenzy. This is surprisingly boring for a Troma release.<br /><br />So please do yourself a favor, and skip this movie. If you have to see it you will understand why.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"In Bollywood it isn't rare that worthless films become hits, good films flopping and good actors not making it big<br /><br />AKS is such a movie<br /><br />Himesh after a music director and singer tried acting Hell man, just because his songs became a hit that means next he becomes an actor<br /><br />The producers were sure the film will work perhaps, the songs were a hit too and of course Himesh did his cheap publicity as usual<br /><br />The film tells such a poor story, such poor direction, such poor acting it makes you cringe<br /><br />Indian rickshaws in Germany, Stunts by Himesh and lot of stupidity Himesh's cap is intact even when he is in the car which somersaults<br /><br />Direction is poor Music is saving grace though most songs sound the same<br /><br />Himesh tries hard but sadly his emotive scenes are a joke, lacks expressions, he is best suited for his music director and some singing He cuts a sorry picture Hansika is awful Malika is okay Sachin Khedekar is okay, Darshan Jhariwala hams\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in test_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "valid_dataset = valid_pos_reviews.concatenate(valid_neg_reviews).shuffle(buffer_size = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch = valid_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_pos_reviews.concatenate(test_neg_reviews).shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing =test_dataset.map(lambda X,y: \n",
    "                                (text_to_integers(string_transform(X)), y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(290,), dtype=int64, numpy=\n",
       " array([68165, 67404, 40322, 53730,  7234, 39897,  1952, 53730, 47496,\n",
       "        14407, 55549,   135, 51457,  3443,  8482, 68165, 67404, 24156,\n",
       "        54077, 64890, 63680, 29750, 64318,   904, 51749, 14407, 60922,\n",
       "        54668, 68165,  6538, 50021, 36853, 38757, 54738,  2613, 55572,\n",
       "        64049, 14407,  7034, 54077, 69880, 55362, 14407, 38757, 54738,\n",
       "         7034, 54192, 11251,  6631, 14407, 26291,  3809,  7034, 74268,\n",
       "        66042, 27334, 65225,  8278, 19720, 28164, 51766,  3809, 26678,\n",
       "        46971, 27495, 74253, 68165, 27927, 55498, 22149,  6626,  7234,\n",
       "         6026, 36228, 29302, 23242, 27927, 27927, 13219, 54609, 14912,\n",
       "        71133,  7234, 58408, 60852,  3809, 44613, 74942, 21260, 51385,\n",
       "         9114, 12398, 53730, 25838, 68087,  7234, 17744, 32230, 24932,\n",
       "        12509,  3809, 73108,  4024, 45023, 72608, 44352, 67509, 17837,\n",
       "         8278,  3809, 38195, 65225,  1030, 44992, 67509, 52274, 43920,\n",
       "        68165, 57503, 67404, 46665,  3809, 44613,  4556,  4822, 20030,\n",
       "        20163, 40384, 61764, 10854, 46665,  3809, 23071, 46269,  5559,\n",
       "        28164, 72608, 67404, 24156, 40322, 53730,  7234, 34843, 60906,\n",
       "        46724, 64318, 42347, 36333, 42762, 31566, 72608, 30094, 29750,\n",
       "        72608, 60922, 55549, 74282, 45986, 31050, 44930, 67576, 50021,\n",
       "        23242, 53985, 37989, 51457,  7234, 43627, 73383, 14407, 22538,\n",
       "         9519, 29302, 72608, 38254, 54948, 51457,  7234, 51703, 37989,\n",
       "        43792, 67509,  4024, 31566, 14407,  6828, 28164, 27927, 35046,\n",
       "         7234, 64852, 24876, 18473,  8278, 60585, 28164, 27927,  5559,\n",
       "        23242, 27927, 29750, 27261,  7234, 43627, 13225,  5525,  7234,\n",
       "        13219, 53492, 31566, 54609, 67343, 51457,  7234, 17744, 32230,\n",
       "        27452,  7234, 21001, 13272, 62527, 27927, 42413, 39287,  5559,\n",
       "        21001,  8078, 17880, 55549, 29433, 56144, 29419, 19624, 66872,\n",
       "        66872, 30555, 19624,  9114, 74282, 31566, 37053, 36934, 29302,\n",
       "        57093, 63028, 66491, 41950, 69751,  7234,  8482, 51457, 52128,\n",
       "        34616, 55362, 28164, 67404, 18757,  7234, 38358, 27927, 29750,\n",
       "        73659, 14407,  7034, 14288, 14407,  7034, 30582, 74253, 68165,\n",
       "        27927, 40530,  1963, 29206, 72085,  9114, 11880, 53730, 74301,\n",
       "        32741, 28164, 14407, 52128,   904, 54738, 44613, 26727,  3809,\n",
       "         8773, 55498])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = next(iter(testing))\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
