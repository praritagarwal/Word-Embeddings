{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create and train an embedding layer for the words appearing in a text data. We will then train a simple DNN based model to do sentiment analysis on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exercise 13.10 in [this](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "\n",
    "  - a. Download the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
    "  \n",
    "    \n",
    "  - b. Split the test set into a validation set (15,000) and a test set (10,000).\n",
    "  \n",
    "  \n",
    "  - c. Use tf.data to create an efficient dataset for each set.\n",
    "  \n",
    "  \n",
    "  - d. Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method.\n",
    "  \n",
    "  \n",
    "  - e. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n",
    "  \n",
    "  \n",
    "  - f. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.\n",
    "\n",
    "\n",
    "  - g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.1.0\n",
      "keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/prarit/MachineLearningProjects/Word-Embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('cwd: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good tutorial on using wget: https://www.tecmint.com/download-and-extract-tar-files-with-one-command/\n",
    "# turn off verbose output of wget using the flag -nv : https://shapeshed.com/unix-wget/#how-to-turn-off-verbose-output \n",
    "!wget -c -nv http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -o - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncompress the downloaded files\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tensorflow also provides this dataset: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md',\n",
       " '.gitignore',\n",
       " 'log_dir',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'aclImdb',\n",
       " 'Word-Embeddings.ipynb',\n",
       " 'aclImdb_v1.tar.gz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the files in the current working directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that aclImdb_v1.tar.gz was extracted to a folder called aclImdb. Let's see the contents of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of aclImdb are: \n",
      "['imdb.vocab', 'train', 'README', 'imdbEr.txt', 'test']\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd() , 'aclImdb')\n",
    "contents = os.listdir(path)\n",
    "print('The contents of aclImdb are: \\n{}'.format(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a README file in aclImdb, let us read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read README\n",
    "filepath = os.path.join(path, 'README')\n",
    "with open(filepath, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From README, we find that the train/test folder contains a 'pos' folder for positive reviews and a 'neg' for negative reviews along with .txt files containing urls of positive and negative reviews respectively. There are also some other files for bag-of-words features etc. \n",
    "\n",
    "Each train/test folder contains a total of 25000 reviews of which 12500 are positive reviews and 12500 are negative reviews.\n",
    "\n",
    "Let's verify the above about the 'train' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents of the train folder: \n",
      "['unsup', 'urls_pos.txt', 'pos', 'labeledBow.feat', 'unsupBow.feat', 'neg', 'urls_neg.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(path,'train')\n",
    "print('contents of the train folder: \\n{}'.format(os.listdir(train_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a dataset, we need the path to all the reviews. We can create the corresponding list of paths using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with positive reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the positive reviews in the training set\n",
    "train_pos_path = os.path.join(train_path, 'pos', '*.txt')\n",
    "train_pos_reviews = glob.glob(train_pos_path)\n",
    "print('No. of train-set files with positive reviews: {}'.format(len(train_pos_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train-set files with negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# paths to the negative reviews in the training set\n",
    "train_neg_path = os.path.join(train_path, 'neg', '*.txt')\n",
    "train_neg_reviews = glob.glob(train_neg_path)\n",
    "print('No. of train-set files with negative reviews: {}'.format(len(train_neg_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us give a brief look at a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I watch them all.<br /><br />It's not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it's completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don't watch. 'How she move.' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It's just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I'm a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn't all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.\n"
     ]
    }
   ],
   "source": [
    "file = train_pos_reviews[0]\n",
    "with open(file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will like to load an preprocess all the data using tensorflow's data API, therefore let us quickly see how to read the same file as above but this time by using tensorflow's [TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'I watch them all.<br /><br />It\\'s not better than the amazing ones (_Strictly Ballroom_, _Shall we dance?_ (Japanese version), but it\\'s completely respectable and pleasingly different in parts.<br /><br />I am an English teacher and I find some of the ignorance about language in some of these reviews rather upsetting. For example: the \"name should scream don\\'t watch. \\'How she move.\\' Since when can movie titles ignore grammar?\" <br /><br />There is nothing inherently incorrect about Caribbean English grammar. It\\'s just not Canadian standard English grammar. Comments about the dialogue seem off to me. I put on the subtitles because I\\'m a Canadian standard English speaker, so I just AUTOMATICALLY assumed that I would have trouble understanding all of it. It wasn\\'t all that difficult and it gave a distinctly different flavour as the other step movies I have seen were so American.<br /><br />I loved that this movie was set in Toronto and, in fact, wish it was even more clearly set there. I loved that the heroine was so atypically cast. I enjoyed the stepping routines. I liked the driven Mum character. I felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies.<br /><br />In summary, if you tend to like dance movies, then this is a decent one. If you have superiority issues about the grammar of the English standard you grew up speaking, your narrow mind may have difficulty enjoying this movie.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "pos_fl0 = tf.data.TextLineDataset(file)\n",
    "for item in pos_fl0:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As expected, we see that the pos_fl0 contains a single item and it's value output matches the output of the previous code cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having, learnt how to use tf.data.TextLineData() method, we can now starting preprocessing the data. In order to do this, we notice that the review contains punctuation marks and html line break tags etc. We will have to write a preprocessing function to get rid of these. Additionally, we will also change all alphabets to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing line-brk tags\n",
    "\n",
    "This can be very simply done by using the .replace() method of python strings. We can therefore use it to replace all occurrances of the line-break tag with a space. In tensorflow, the equivalent method is [tf.strings.regex_replace()](https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.strings.regex_replace\n",
    "Note that 'regex' in regex_replace() stands for [\"regular expression\"](https://docs.python.org/3/howto/regex.html). For e.g. the following will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('hello','e','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the following will throw an error: tf.strings.regex_replace('h(llo', '(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because \"(\" is a metacharacter. To match and replace meta-characters, we must prepend a backslash before them. This can be done as follows: '\\\\\\\\' + char. \n",
    "\n",
    "The previous code cell can now be made to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The error thrown by this code cell is intentional\n",
    "tf.strings.regex_replace('h(llo', '\\\\'+'(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h(llo','\\(','E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imp: \n",
    "\n",
    "Note that backslash itself is also a  meta-character. To search and replace backslash, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.regex_replace('h\\llo', '\\\\'+'\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hEllo'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use '\\\\\\\\' and NOT '\\\\' to search and replace a backslash\n",
    "tf.strings.regex_replace('h\\llo', '\\\\\\\\', 'E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try three different ways of removing punctuations from a tensorflow string and compare their timings:\n",
    "\n",
    "1) punc_filter_and_to_lower1: use tf.strings.unicode_decode() to convert all the characters in the string into an array of their ascii codes. We then loop through this array, skipping over the places where the entry matches the ascii code of a punctuation. Finally we call tf.strings.unicode_encode() on this array to convert the ascii codes back to characters, thereby obtaining a string with all the punctuations stripped. \n",
    "\n",
    "\n",
    "2) punc_filter_and_to_lower2: use tf.regex_replace() to search and replace each punctuation by empty space. Pay special attention to prepend backslash in order to be able to use meta-characters in regex_replace.\n",
    "\n",
    "\n",
    "3) punc_filter_and_to_lower3: simply extract the python string using its .numpy() method. Then simple iterate through the characters of the string, skipping over the punctutations. Join the resulting list of character using .join() method. \n",
    "\n",
    "4) punc_filter_and_to_lower4: Use [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) by setting it's \"alphanum_only\" arg to True. This way it will only parse alpha-numeric characters in the text and to split the text at occurrances of non-alphanumeric characters. Since whitespace is a non-alpha-numeric character, the output will largely consist of a list of words in the text.  The caeat with this approach is that words with apostrophe in them such as \" don't \" will be split into two words: \"don\" and \"t\". Also, [tfds.features.text.Tokenizer()](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) does NOT consider underscores as non-alpha-numeric, so underscores do not get removed form the text. On the other hand since, the tokenizer already generates a list of words, we use this list to generate a vocobulary of words in the dataset at this step it-self, making the preprocessing faster.\n",
    "\n",
    "There timing on the first review in the training set was as follows:\n",
    "\n",
    "1) punc_filter_and_to_lower1: Wall time: ~ 3 s\n",
    "\n",
    "2) punc_filter_and_to_lower2: Wall time: ~ 8 ms\n",
    "\n",
    "3) punc_filter_and_to_lower3: Wall time: ~ 6 ms\n",
    "\n",
    "4) punc_filter_and_to_lower4: Wall time: ~ 6 ms (when not udating a vocabulary of words) \n",
    "\n",
    "5) 4) punc_filter_and_to_lower4: Wall time: ~ 7 ms (when also udating a vocabulary of words) \n",
    "\n",
    "Clearly, the last function is the fastest with the first one being extremely slow (takes several secs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations before utf encoding: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "punctuations after utf encoding: \n",
      "[ 33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  58  59  60\n",
      "  61  62  63  64  91  92  93  94  95  96 123 124 125 126]\n"
     ]
    }
   ],
   "source": [
    "# list of punctuations\n",
    "punc_ls = string.punctuation\n",
    "print('punctuations before utf encoding: {}'.format(punc_ls))\n",
    "punc_ls2 = tf.strings.unicode_decode(punc_ls, input_encoding = 'UTF-8')\n",
    "print('punctuations after utf encoding: \\n{}'.format(punc_ls2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.unicode_encode(), tf.strings.unicode_decode()\n",
    "def punc_filter_and_to_lower1(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = tf.strings.unicode_decode(st2, input_encoding = 'utf-8')\n",
    "    st2 = tf.strings.unicode_encode([char for char in st2 if char not in punc_ls2], output_encoding = 'UTF-8')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.41 s, sys: 71.7 ms, total: 3.48 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower1(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tf.strings.regex_replace()\n",
    "def punc_filter_and_to_lower2(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ')\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    for punc in punc_ls:\n",
    "        # V.imp: to replace meta-characters we prepend a backslash to them \n",
    "        # infact we can also prepend a backslach before all the characters\n",
    "        # to prepend a backslash before a character we do: '\\\\' + char\n",
    "        st2 = tf.strings.regex_replace(st2, '\\\\'+ punc, ' ')\n",
    "    \n",
    "    st2 = tf.strings.lower(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 ms, sys: 4.02 ms, total: 8.17 ms\n",
      "Wall time: 7.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower2(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all  it s not better than the amazing ones   strictly ballroom    shall we dance    japanese version   but it s completely respectable and pleasingly different in parts  i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting  for example  the  name should scream don t watch   how she move   since when can movie titles ignore grammar    there is nothing inherently incorrect about caribbean english grammar  it s just not canadian standard english grammar  comments about the dialogue seem off to me  i put on the subtitles because i m a canadian standard english speaker  so i just automatically assumed that i would have trouble understanding all of it  it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american  i loved that this movie was set in toronto and  in fact  wish it was even more clearly set there  i loved that the heroine was so atypically cast  i enjoyed the stepping routines  i liked the driven mum character  i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies  in summary  if you tend to like dance movies  then this is a decent one  if you have superiority issues about the grammar of the english standard you grew up speaking  your narrow mind may have difficulty enjoying this movie '>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using .join() method in python string class\n",
    "def punc_filter_and_to_lower3(st):\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.regex_replace(st, line_brk_tag, ' ') # regex stands for regular expressions\n",
    "                                                          # i.e. they are not metacharacters\n",
    "                                                          # https://docs.python.org/3/howto/regex.html#matching-characters\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    st2 = st2.numpy().decode('utf-8')\n",
    "    st2 = ''.join([char for char in st2 if char not in punc_ls])\n",
    "    \n",
    "    st2 = st2.lower()\n",
    "    \n",
    "    st2 = tf.convert_to_tensor(st2)\n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.3 ms, sys: 0 ns, total: 5.3 ms\n",
      "Wall time: 4.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower3(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all its not better than the amazing ones strictly ballroom shall we dance japanese version but its completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream dont watch how she move since when can movie titles ignore grammar  there is nothing inherently incorrect about caribbean english grammar its just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because im a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasnt all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rid of punctuations, html line breaks and change to lower case\n",
    "# using tfds.features.text.tokenizer()\n",
    "# we will also simultaneously generate a vocabulary in this step\n",
    "import tensorflow_datasets as tfds\n",
    "def punc_filter_and_to_lower4(st, vocab = None):\n",
    "    ''' vocab: vocabulary to update with the words in st'''\n",
    "    \n",
    "    # before removing punctuations, we should remove the html line-break tag\n",
    "    # this is because the line break tag contains <,/ and > characters which \n",
    "    # will be removed if we remove punctuations first. This will then make it harder \n",
    "    # to identify the line-break-tag\n",
    "    line_brk_tag = \"<br /><br />\" \n",
    "    st2 = tf.strings.lower(tf.strings.regex_replace(st, line_brk_tag, ' '))\n",
    "    \n",
    "    # now we replace all the punctions in the string\n",
    "    tokenizer = tfds.features.text.Tokenizer()\n",
    "    words = tokenizer.tokenize(st2.numpy()) # note that the inpout has to be a python string NOT as tensor \n",
    "                                            # The output is a list NOT a tensor\n",
    "                                            # https://stackoverflow.com/questions/56665868/tensor-numpy-not-working-in-tensorflow-data-dataset-throws-the-error-attribu\n",
    "    st2 = tf.strings.join(words, ' ')\n",
    "    \n",
    "    if type(vocab) == set:\n",
    "        vocab.update(words)\n",
    "        return st2, vocab\n",
    "    \n",
    "    \n",
    "    return st2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.45 ms, sys: 0 ns, total: 7.45 ms\n",
      "Wall time: 6.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    lst = punc_filter_and_to_lower4(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'i watch them all it s not better than the amazing ones _strictly ballroom_ _shall we dance _ japanese version but it s completely respectable and pleasingly different in parts i am an english teacher and i find some of the ignorance about language in some of these reviews rather upsetting for example the name should scream don t watch how she move since when can movie titles ignore grammar there is nothing inherently incorrect about caribbean english grammar it s just not canadian standard english grammar comments about the dialogue seem off to me i put on the subtitles because i m a canadian standard english speaker so i just automatically assumed that i would have trouble understanding all of it it wasn t all that difficult and it gave a distinctly different flavour as the other step movies i have seen were so american i loved that this movie was set in toronto and in fact wish it was even more clearly set there i loved that the heroine was so atypically cast i enjoyed the stepping routines i liked the driven mum character i felt that many of the issues in the movie were addressed more subtly than is characteristic of dance movies in summary if you tend to like dance movies then this is a decent one if you have superiority issues about the grammar of the english standard you grew up speaking your narrow mind may have difficulty enjoying this movie'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.99 ms, sys: 0 ns, total: 7.99 ms\n",
      "Wall time: 7.52 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test the timing of punc_filter_and_to_lower4 when simultaneously updating a dictionary\n",
    "vocab2 = set([])\n",
    "for item in pos_fl0:\n",
    "    lst, vocab2 = punc_filter_and_to_lower4(item, vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary based on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be easily done using python's [set()](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset) container.: basically, split each text in the training instance into words and update the set with this list. This will add any new words in the text to the set. In the end, the set will contain all the unique words in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have also implemented the above idea in our function punc_filter_and_to_lower4() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a vocabulary using the set() container\n",
    "vocab1 = set([]) # python set for containing unique words in the training dataset\n",
    "def vocab_builder1(strng):\n",
    "    # split the string into its words\n",
    "    words = tf.strings.split(strng)\n",
    "    # update vocab\n",
    "    vocab1.update(words.numpy())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 268 ms, sys: 63.9 ms, total: 331 ms\n",
      "Wall time: 330 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower3(item)\n",
    "    vocab_builder1(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocab_builder1() function as defined above takes about 300 ms. On the otherhand, in punc_filter_and_to_lower4() function,  we were able to get almost the same result (upto the caveats mentioned in the previous section) by using tfds.features.text.Tokenizer(), in about 8 ms. Clearly, there is a difference of few order of magnitude between the time taken by the two routines with the difference between their output being quite tolerable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================="
     ]
    }
   ],
   "source": [
    "vocab = set([])\n",
    "train_filepaths = tf.data.Dataset.list_files([train_pos_path, train_neg_path])\n",
    "train_dataset = tf.data.TextLineDataset(train_filepaths)\n",
    "\n",
    "ctr = 0\n",
    "for item in train_dataset:\n",
    "    _ , vocab = punc_filter_and_to_lower4(item, vocab)\n",
    "    \n",
    "    ctr+=1\n",
    "    if ctr%255 ==0:\n",
    "        print(\"=\", end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 5 random words from vocab to inspect it\n",
    "import random\n",
    "\n",
    "random.sample(vocab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('no. of words in vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a lookup table based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "lookup_initializer = tf.lookup.KeyValueTensorInitializer( list(vocab), indices) \n",
    "num_oov_buckets = 50 \n",
    "lookup_table = tf.lookup.StaticVocabularyTable( initializer = lookup_initializer, \n",
    "                                               num_oov_buckets = num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map a text to integers using the above lookup table\n",
    "def text_to_integers(X):\n",
    "    X = tf.strings.split(X)\n",
    "    int_ls = lookup_table.lookup(X)\n",
    "    return int_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "initial_embeddings = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim] ) \n",
    "embedding_matrix = tf.Variable(initial_embeddings)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an embedding-vector from a list of word indices\n",
    "def indices_to_embeddings(idxs):\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix, idxs)\n",
    "    sqrt_num_words = tf.math.sqrt(tf.cast(tf.size(idxs), tf.float32)) # size is of type int32 but \n",
    "                                                                      # tf.math.sqrt has to have one of \n",
    "                                                                      # bfloat16, half, float32, \n",
    "                                                                      # float64, complex64, \n",
    "                                                                      # complex128.\n",
    "    mean_embedding = tf.math.reduce_mean(embeddings, axis = 0)\n",
    "    sentence_embedding = tf.multiply(sqrt_num_words, mean_embedding)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in pos_fl0:\n",
    "    item = punc_filter_and_to_lower4(item)\n",
    "    item = text_to_integers(item)\n",
    "    item = indices_to_embeddings(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessing layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a preprocessing layer that accepts a sentence as an input, removes punctuations and line-brk tags form it, then converts it's words into indices and finally output an embedding vector corresponding to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_layer(keras.layers.Layer): \n",
    "    def __init__(self, vocab,  num_oov_buckets, embedding_len, kernel_regularizer = None, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab = list(vocab)\n",
    "        self.vocab_len  = tf.size(self.vocab, out_type = tf.int64) \n",
    "        self.num_oov_buckets = num_oov_buckets\n",
    "        self.embedding_len = embedding_len\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.indices = tf.range(self.vocab_len, dtype = tf.int64)\n",
    "        self.lookup_initializer = tf.lookup.KeyValueTensorInitializer(self.vocab, \n",
    "                                                                      self.indices)\n",
    "        self.lookup_table = tf.lookup.StaticVocabularyTable(initializer = self.lookup_initializer, \n",
    "                                                            num_oov_buckets = self.num_oov_buckets)\n",
    "         \n",
    "    # build the embedding matrix \n",
    "    def build(self, batch_input_shape):\n",
    "        self.embedding_mat = self.add_weight(name = 'embedding matrix', \n",
    "                                             shape = [self.vocab_len + self.num_oov_buckets, self.embedding_len],\n",
    "                                             initializer = tf.random_uniform_initializer)\n",
    "        \n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    \n",
    "    # function to convert a sentence into a list of indices for its words\n",
    "    def text_to_integers(self, X):\n",
    "        X_words = tf.strings.split(X)\n",
    "        # X_words will be a ragged tensor\n",
    "        # on the otherhand, self.lookup_table.lookup() expects either a Sparse tensor or a dense tensor\n",
    "        # for example see the documentation: https://www.tensorflow.org/api_docs/python/tf/lookup/StaticVocabularyTable\n",
    "        # in order to use it ragged tensors, we have to use tf.ragged.map_flay_values()\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/ragged/map_flat_values\n",
    "        X_int = tf.ragged.map_flat_values(self.lookup_table.lookup, X_words)\n",
    "        return X_int\n",
    "    \n",
    "    # function to convert a list of indices into an embedding vector\n",
    "    def integers_to_embeddings(self, X):\n",
    "        sqrt_num_words = tf.linalg.diag(\n",
    "            tf.math.sqrt(tf.cast(X.nested_row_lengths(), tf.float32))[0])\n",
    "        X_ems = tf.nn.embedding_lookup(self.embedding_mat, X)\n",
    "        X_vec = tf.math.reduce_mean(X_ems, axis = 1) # note that X_ems is a ragged tensor whose \n",
    "                                                     # 0-th dim. corresponds to the batch dim. \n",
    "                                                     # therefore axis = 1 and NOT 0 when taking mean\n",
    "        \n",
    "        return sqrt_num_words@X_vec\n",
    "        \n",
    "    # define the call method\n",
    "    def call(self, X):\n",
    "        \n",
    "        # recall X contains strings with all the punctuations etc removed\n",
    "        # Imp: The shape of X is (batch_size, ...) i.e. the 0-th axis of X corresponds to batch dim. \n",
    "        # We first convert all its words into their indices\n",
    "        X_int = self.text_to_integers(X)\n",
    "        # now convert this a mean embedding vector\n",
    "        X_vec = self.integers_to_embeddings(X_int)\n",
    "        \n",
    "        if self.kernel_regularizer:\n",
    "            self.add_loss(self.kernel_regularizer(self.embedding_mat))\n",
    "        \n",
    "        return X_vec\n",
    "    \n",
    "    \n",
    "    # compute output shape\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.embedding_len])\n",
    "    \n",
    "    \n",
    "    # add the layers hyperparameters to the configuration file\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'embedding_len': self.embedding_len, \n",
    "                'num_oov_buckets': self.num_oov_buckets}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the positive and negative reviews in each dataset have been stored in seperate folders and as such they have no explicit labels. We will therefore have to create our own labels as was done in [this](https://www.tensorflow.org/tutorials/load_data/text) official tensorflow tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return apply a label\n",
    "def labeler(review, label):\n",
    "    return review, tf.cast(label, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_paths = tf.data.Dataset.list_files(train_pos_path)\n",
    "train_pos_reviews = tf.data.TextLineDataset(train_pos_paths).map(lambda x : labeler(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_paths = tf.data.Dataset.list_files(train_neg_path)\n",
    "train_neg_reviews = tf.data.TextLineDataset(train_neg_paths).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_buffer_size = 20000\n",
    "train_dataset = train_pos_reviews.concatenate(train_neg_reviews).shuffle(buffer_size = shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for item in train_dataset:\n",
    "    count+=1\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to apply punc_filter_and_to_lower4() to the reviews in our training dataset. For this we will use the [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method of tf.data.Datasets. At this point it is important to note that (as mentioned in the documentation for [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)):\n",
    "\n",
    "    Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have two options:\n",
    "\n",
    "     1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.\n",
    "\n",
    "     2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is of concern to us because the [tokenizer](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer) we used in punc_filter_and_to_lower4() only accepts string input and NOT tensors. This string was extracted from the tensor using its .numpy() method which is only operable in eager mode. Thus when we pass punc_filter_and_to_lower4() to map(), it throws an error, complaining:\n",
    "\n",
    "     'Tensor' object has no attribute 'numpy'\n",
    "\n",
    "\n",
    "Thus we have to wrap punc_filter_and_to_lower4() with tf.py_function before passing it to map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to wrap punc_filter_and_to_lower4 with tf.py_function\n",
    "# we will also use tf.ensure_shape(), or else tensorflow is \n",
    "# unable to statically determine it, leading to error being thrown during training\n",
    "def string_transform(X):\n",
    "    x = tf.py_function(punc_filter_and_to_lower4, [X], Tout = tf.string)\n",
    "    x = tf.ensure_shape(x, ())\n",
    "    \n",
    "    return x\n",
    "\n",
    "batch_size = 50\n",
    "prefetch = 2\n",
    "train_batch = train_dataset.map(lambda X, y: \n",
    "                                    (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have alternatively, preprocessed the data using punc_filter_and_to_lower2() which is based on tensorflow functions and then used it. The price we would have to pay is to rebuild the vocabulary accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ite = next(iter(train_batch))\n",
    "print('shape of train-batch: {}'.format(ite[0].shape))\n",
    "print(ite[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ite[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed in the excercise, we will create a validation dataset by splitting the test set into half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(path, 'test')\n",
    "print(test_path)\n",
    "test_pos_path = os.path.join(test_path, 'pos', '*.txt')\n",
    "print('path to positive test reviews: \\n{}'.format(test_pos_path))\n",
    "test_neg_path = os.path.join(test_path, 'neg', '*.txt')\n",
    "print('path to negative test reviews: \\n{}'.format(test_neg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_files = glob.glob(test_pos_path)\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 12500\n",
    "valid_pos_files = random.sample(test_pos_files, int(valid_size/2))\n",
    "len(valid_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can remove the valid_pos_files from the set of test_pos_files through the\n",
    "# simple trick explained in the following post:\n",
    "# https://stackoverflow.com/questions/6486450/python-compute-list-difference/6486467\n",
    "test_pos_files = list(set(test_pos_files) - set(valid_pos_files))\n",
    "len(test_pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_files = glob.glob(test_neg_path)\n",
    "print('initial test_neg_files len:{}'.format(len(test_neg_files)))\n",
    "valid_neg_files = random.sample(test_neg_files, int(valid_size/2))\n",
    "print('valid_neg_files len: {}'.format(len(valid_neg_files)))\n",
    "test_neg_files = list(set(test_neg_files) - set(valid_neg_files))\n",
    "print('test_neg_files len: {}'.format(len(test_neg_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pos_reviews = tf.data.TextLineDataset(valid_pos_files).map(lambda x: labeler(x, 1))\n",
    "valid_neg_reviews = tf.data.TextLineDataset(valid_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in valid_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in valid_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_reviews = tf.data.TextLineDataset(test_pos_files).map(lambda x: labeler(x, 1))\n",
    "test_neg_reviews = tf.data.TextLineDataset(test_neg_files).map(lambda x: labeler(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in test_pos_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in test_neg_reviews.take(2):\n",
    "    print(item)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "valid_dataset = valid_pos_reviews.concatenate(valid_neg_reviews).shuffle(buffer_size = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch = valid_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_pos_reviews.concatenate(test_neg_reviews).shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_dataset.map(lambda X,y: \n",
    "                                (string_transform(X), y) ).batch(batch_size).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple model to test the preprocessing layer defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, PReLU\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 50\n",
    "embedding_len = 100\n",
    "weight_reg = 0.01\n",
    "kernel_regularizer = regularizers.l2(weight_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = [], dtype = tf.string))\n",
    "model.add(Preprocessing_layer(vocab = vocab, num_oov_buckets = num_oov_buckets, \n",
    "                              embedding_len = embedding_len, \n",
    "                              kernel_regularizer = kernel_regularizer) )\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 256, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 256, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 64, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 64, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 16, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 4, activation = None))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard logs\n",
    "import time\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'log_dir')\n",
    "print('log directory: \\n{}'.format(log_dir))\n",
    "run_id = time.strftime('%Y_m_%d_%H_%M_%S')\n",
    "run_file = os.path.join(log_dir, run_id)\n",
    "print('run file: \\n{}'.format(run_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for TensorBoard\n",
    "# check the following tutorial\n",
    "# https://www.tensorflow.org/tensorboard/get_started\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "monitor = 'val_loss'\n",
    "min_delta = 0.01\n",
    "patience = 10\n",
    "verbose = 1\n",
    "restore_best_weights = True\n",
    "stopper = EarlyStopping(monitor = monitor, min_delta = min_delta, \n",
    "                        patience = patience, verbose = verbose,\n",
    "                        restore_best_weights = restore_best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fit the model\n",
    "epochs = 50\n",
    "history = model.fit(train_batch, epochs = epochs, \n",
    "                    validation_data = valid_batch, \n",
    "                    callbacks = [stopper, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
